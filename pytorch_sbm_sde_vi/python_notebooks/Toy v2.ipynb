{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hack to import from a parent directory\n",
    "import sys\n",
    "path = '..'\n",
    "if path not in sys.path:\n",
    "    sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python-related imports\n",
    "import math, sys\n",
    "from typing import Dict, Tuple, Union\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "\n",
    "#Torch-related imports\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch import nn\n",
    "import torch.distributions as D\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#Module imports\n",
    "from SBM_SDE_classes import *\n",
    "from obs_and_flow import *\n",
    "from training import *\n",
    "from plotting import *\n",
    "from mean_field import *\n",
    "from TruncatedNormal import *\n",
    "from LogitNormal import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_flow = 1.0 # [1.0, 0.5]\n",
    "t = 1000 # [1000, 5000]\n",
    "x0_SCON = [75, 1.4, 5.5] # [[65, 0.4, 2.5], [75, 1.4, 5.5]]\n",
    "priors_file = '../generated_data/SCON-C_CO2_fix_x_logit_alt_x0_sample_y_t_1000_dt_0-01_sd_scale_0-333_hyperparams.pt'\n",
    "x_path = '../generated_data/SCON-C_CO2_fix_x_logit_alt_x0_sample_y_t_1000_dt_0-01_sd_scale_0-333_x.pt'\n",
    "theta_file = '../generated_data/SCON-C_CO2_fix_x_logit_alt_x0_sample_y_t_1000_dt_0-01_sd_scale_0-333_rsample.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "torch.set_printoptions(precision = 8)\n",
    "\n",
    "#Neural SDE parameters\n",
    "n = int(t / dt_flow) + 1\n",
    "t_span = np.linspace(0, t, n)\n",
    "t_span_tensor = torch.reshape(torch.Tensor(t_span), [1, n, 1]).to(active_device) #T_span needs to be converted to tensor object. Additionally, facilitates conversion of I_S and I_D to tensor objects.\n",
    "\n",
    "#SBM temperature forcing parameters\n",
    "temp_ref = 283\n",
    "temp_rise = 5 #High estimate of 5 celsius temperature rise by 2100.\n",
    "\n",
    "#Specify desired SBM SDE model type and details.\n",
    "state_dim_SCON = 3 #Not including CO2 in STATE_DIM, because CO2 is an observation.\n",
    "SBM_SDE_class = 'SCON'\n",
    "diffusion_type = 'C'\n",
    "theta_dist = 'RescaledLogitNormal' #String needs to be exact name of the distribution class. Other option is 'RescaledLogitNormal'.\n",
    "theta_post_dist = 'MultivariateLogitNormal'\n",
    "\n",
    "#Load logit-normal priors\n",
    "SCON_C_priors_details = {k: v.to(active_device) for k, v in torch.load(priors_file).items()}\n",
    "\n",
    "#Initial condition prior means\n",
    "x0_error_scale = 0.1\n",
    "x0_SCON_tensor = torch.tensor(x0_SCON).to(active_device)\n",
    "x0_prior_SCON = D.multivariate_normal.MultivariateNormal(x0_SCON_tensor,\n",
    "                                                         scale_tril = torch.eye(state_dim_SCON).to(active_device) * x0_error_scale * x0_SCON_tensor)\n",
    "\n",
    "#Generate exogenous input vectors.\n",
    "#Obtain temperature forcing function.\n",
    "temp_tensor = temp_gen(t_span_tensor, temp_ref, temp_rise).to(active_device)\n",
    "\n",
    "#Obtain SOC and DOC pool litter input vectors for use in flow SDE functions.\n",
    "i_s_tensor = i_s(t_span_tensor).to(active_device) #Exogenous SOC input function\n",
    "i_d_tensor = i_d(t_span_tensor).to(active_device) #Exogenous DOC input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class MeanField(nn.Module):\n",
    "#\n",
    "#    '''\n",
    "#    Class for mean-field variational inference of SBM SDE parameters.\n",
    "#    Takes dictionary of parameter distribution information in order of mean, sdev, upper bound, and lower bound.\n",
    "#\n",
    "#    The forward method returns dict_out, samples, log_q_theta, dict_parent_loc_scale.\n",
    "#    dict_out: dictionary of theta samples corresponding to their parameter keys for debugging and printing.\n",
    "#    samples: tensor of the theta values themselves for inference use.\n",
    "#    log_q_theta: log probability of the theta values corresponding to the variational distribution.\n",
    "#    dict_parent_loc_scale: dictionary of the values of the parent loc and scale distribution parameters.\n",
    "#\n",
    "#    Formerly, the forward method returned dict_mean_sd, a dictionary of the values of the transformed distribution means and standard deviations, but calculation of the distribution mean and standard deviation was computationally wasteful to compute at every iteration and simpler for the TruncatedNormal distribution class than RescaledLogitNormal. The means and standard deviations can be computed from dict_parent_loc_scale.\n",
    "#    '''\n",
    "#\n",
    "#    def __init__(self, DEVICE, PARAM_NAMES, INIT_DICT, DIST_CLASS, LEARN_COV=False):\n",
    "#        super().__init__()\n",
    "#        #Use param dict to intialise the means for the mean-field approximations.\n",
    "#        #init_params: name -> (parent mean, parent sd, true lower, true upper)\n",
    "#        means = []\n",
    "#        sds = []\n",
    "#        lower_bounds = []        \n",
    "#        upper_bounds = []\n",
    "#        for key in PARAM_NAMES:\n",
    "#            mean, sd, lower, upper = INIT_DICT[key]\n",
    "#            means.append(mean)\n",
    "#            sds.append(sd)\n",
    "#            upper_bounds.append(upper)\n",
    "#            lower_bounds.append(lower)\n",
    "#\n",
    "#        self.dist = DIST_CLASS\n",
    "#        self.learn_cov = LEARN_COV\n",
    "#        self.means = nn.Parameter(torch.Tensor(means).to(DEVICE))\n",
    "#        if LEARN_COV:\n",
    "#            # Since we init post with indep priors, scale_tril = diag(stddev)\n",
    "#            scale_tril = torch.diag(torch.Tensor(sds)).to(DEVICE)\n",
    "#            # Map scale_tril to unconstrained space\n",
    "#            unconstrained_scale_tril = D.transform_to(self.dist.arg_constraints['scale_tril']).inv(scale_tril)\n",
    "#            self.sds = nn.Parameter(unconstrained_scale_tril) # (num_params, num_params)\n",
    "#        else:\n",
    "#            #scale = torch.Tensor(sds).to(DEVICE)\n",
    "#            #unconstrained_scale = D.transform_to(self.dist.arg_constraints['scale']).inv(scale)\n",
    "#            #self.sds = nn.Parameter(unconstrained_scale)\n",
    "#            self.sds = nn.Parameter(torch.Tensor(sds).to(DEVICE)) # (num_params, )\n",
    "#        self.lowers = torch.Tensor(lower_bounds).to(DEVICE)\n",
    "#        self.uppers = torch.Tensor(upper_bounds).to(DEVICE)\n",
    "#        \n",
    "#        #Save keys for forward output.\n",
    "#        self.keys = PARAM_NAMES\n",
    "#\n",
    "#    def forward(self, N = 10): # N should be assigned batch size in `train` function from training.py.\n",
    "#        #Update posterior.\n",
    "#        parent_loc = self.means\n",
    "#        if self.learn_cov:\n",
    "#            parent_scale_tril = D.transform_to(self.dist.arg_constraints['scale_tril'])(self.sds)\n",
    "#            parent_scale = torch.diag(parent_scale_tril) # this is incorrect unless indep, but not used in inference\n",
    "#            q_dist = self.dist(parent_loc, scale_tril=parent_scale_tril, a = self.lowers, b = self.uppers)\n",
    "#        else:\n",
    "#            parent_scale = LowerBound.apply(self.sds, 1e-8)\n",
    "#            #parent_scale = D.transform_to(self.dist.arg_constraints['scale'])(self.sds)\n",
    "#            q_dist = self.dist(parent_loc, parent_scale, a = self.lowers, b = self.uppers)\n",
    "#\n",
    "#        # Sample theta ~ q(theta).\n",
    "#        samples = q_dist.rsample([N]) # (N, num_params)\n",
    "#        \n",
    "#        # Evaluate log prob of theta samples.\n",
    "#        if self.learn_cov:\n",
    "#            log_q_theta = q_dist.log_prob(samples) # (N, )\n",
    "#        else:\n",
    "#            log_q_theta = torch.sum(q_dist.log_prob(samples), -1) # (N, )\n",
    "#        \n",
    "#        # Return samples in same dictionary format.\n",
    "#        dict_out = {} #Define dictionary with n samples for each parameter.\n",
    "#        for key, sample in zip(self.keys, torch.split(samples, 1, -1),):\n",
    "#            dict_out[f'{key}'] = sample.squeeze(1) #Each sample is of shape [n].\n",
    "#        \n",
    "#        dict_parent_loc_scale = {} #Define dictionary to store parent parameter normal distribution means and standard deviations.\n",
    "#        #dict_mean_sd = {} #Define dictionary to store real parameter normal distribution means and standard deviations for TruncatedNormal distribution.\n",
    "#        #real_loc = q_dist.mean #q_dist._mean\n",
    "#        #real_scale = q_dist.stddev #torch.sqrt(q_dist._variance)\n",
    "#        # for key, parent_loc_scale, mean_sd in zip(self.keys, torch.split(torch.stack([parent_loc, parent_scale], 1), 1, 0), torch.split(torch.stack([real_loc, real_scale], 1), 1, 0)):\n",
    "#        #     dict_parent_loc_scale[f'{key}'] = parent_loc_scale\n",
    "#        #     dict_mean_sd[f'{key}'] = mean_sd\n",
    "#        for key, parent_loc_scale in zip(self.keys, torch.split(torch.stack([parent_loc, parent_scale], 1), 1, 0)):\n",
    "#            dict_parent_loc_scale[f'{key}'] = parent_loc_scale\n",
    "#        \n",
    "#        #Return samples in dictionary and tensor format.                                \n",
    "#        #return dict_out, samples, log_q_theta, dict_parent_loc_scale, dict_mean_sd\n",
    "#        return dict_out, samples, log_q_theta, dict_parent_loc_scale\n",
    "#\n",
    "#        #if self.dist == TruncatedNormal:\n",
    "#        #    dict_mean_sd = {} #Define dictionary to store real parameter normal distribution means and standard deviations for TruncatedNormal distribution.\n",
    "#        #    real_loc = q_dist.mean #q_dist._mean\n",
    "#        #    real_scale = q_dist.stddev #torch.sqrt(q_dist._variance)\n",
    "#        #    for key, loc_scale, mean_sd in zip(self.keys, torch.split(torch.stack([parent_loc, parent_scale], 1), 1, 0), torch.split(torch.stack([real_loc, real_scale], 1), 1, 0)):\n",
    "#        #        dict_parent_loc_scale[f'{key}'] = loc_scale\n",
    "#        #        dict_mean_sd[f'{key}'] = mean_sd\n",
    "#        #    #Return samples in dictionary and tensor format.                                \n",
    "#        #    return dict_out, samples, log_q_theta, dict_parent_loc_scale, dict_mean_sd\n",
    "#\n",
    "#        #elif self.dist == RescaledLogitNormal:\n",
    "#        #    for key, loc_scale in zip(self.keys, torch.split(torch.stack([parent_loc, parent_scale], 1), 1, 0)):\n",
    "#        #        dict_parent_loc_scale[f'{key}'] = loc_scale\n",
    "#        #    #Return samples in dictionary and tensor format.                \n",
    "#        #    return dict_out, samples, log_q_theta, dict_parent_loc_scale  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_log_lik(C_PATH, PARAMS_DICT, DT, SBM_SDE_CLASS, INIT_PRIOR):\n",
    "    print(C_PATH.shape) # (batch_size, n, state_dim) -> (batch_size, num_sequences, n, state_dim)\n",
    "    \n",
    "    drift, diffusion_sqrt = SBM_SDE_CLASS.drift_diffusion(C_PATH, PARAMS_DICT) # (batch_size, N-1, state_dim), (batch_size, N-1 or 1, state_dim, state_dim)\n",
    "    print('drift diffusion', drift.shape, diffusion_sqrt.shape)\n",
    "    \n",
    "    euler_maruyama_state_sample_object = D.multivariate_normal.MultivariateNormal(loc = C_PATH[:, :-1, :] + drift * DT, scale_tril = diffusion_sqrt * math.sqrt(DT)) #C_PATH[:, :-1, :] + drift * DT will diverge from C_PATH if C_PATH values not compatible with x0 and theta. Algorithm aims to minimize gap between computed drift and actual gradient between x_n and x_{n+1}. \n",
    "\n",
    "    # Compute log p(x|theta) = log p(x|x0, theta) + log p(x0|theta)\n",
    "    ll = euler_maruyama_state_sample_object.log_prob(C_PATH[:, 1:, :]).sum(-1) # log p(x|x0, theta)\n",
    "    ll += INIT_PRIOR.log_prob(C_PATH[:, 0, :]) # log p(x0|theta), (batch_size, )\n",
    "    # needs to be (batch_size, num_sequences)\n",
    "\n",
    "    return ll, drift, diffusion_sqrt\n",
    "\n",
    "def train(DEVICE, LR, NITER, BATCH_SIZE, X_FILE, T, DT, N,\n",
    "        T_SPAN_TENSOR, I_S_TENSOR, I_D_TENSOR, TEMP_TENSOR, TEMP_REF,\n",
    "        SBM_SDE_CLASS, DIFFUSION_TYPE, X0_PRIOR, PRIOR_DIST_DETAILS_DICT, \n",
    "        THETA_DIST = None, THETA_POST_DIST = None, THETA_POST_INIT = None,\n",
    "        LR_DECAY = 0.8, DECAY_STEP_SIZE = 50000, PRINT_EVERY = 100):\n",
    "    torch.manual_seed(0)\n",
    "    \n",
    "    # Instantiate SBM_SDE object based on specified model and diffusion type.\n",
    "    SBM_SDE_class_dict = {\n",
    "            'SCON': SCON,\n",
    "            'SAWB': SAWB,\n",
    "            'SAWB-ECA': SAWB_ECA\n",
    "            }\n",
    "    if SBM_SDE_CLASS not in SBM_SDE_class_dict:\n",
    "        raise NotImplementedError('Other SBM SDEs aside from SCON, SAWB, and SAWB-ECA have not been implemented yet.')\n",
    "    SBM_SDE_class = SBM_SDE_class_dict[SBM_SDE_CLASS]\n",
    "    SBM_SDE = SBM_SDE_class(T_SPAN_TENSOR, I_S_TENSOR, I_D_TENSOR, TEMP_TENSOR, TEMP_REF, DIFFUSION_TYPE)\n",
    "\n",
    "    # Load x, exclude CO2 and extra time steps \n",
    "    # (since data generation process uses smaller dt than dt_flow)\n",
    "    x_all = torch.load(X_FILE)\n",
    "    step = (x_all.shape[1] - 1) // (N - 1)\n",
    "    x = torch.tensor(x_all[:SBM_SDE.state_dim, ::step], dtype=torch.float)\n",
    "    x = x.T.expand(BATCH_SIZE, -1, -1)\n",
    "    assert x.shape == (BATCH_SIZE, N, SBM_SDE.state_dim)\n",
    "\n",
    "    # Convert prior details dictionary values to tensors.\n",
    "    param_names = list(PRIOR_DIST_DETAILS_DICT.keys())\n",
    "    prior_list = list(zip(*(PRIOR_DIST_DETAILS_DICT[k] for k in param_names))) #Unzip prior distribution details from dictionary values into individual lists.\n",
    "    prior_means_tensor, prior_sds_tensor, prior_lowers_tensor, prior_uppers_tensor = torch.tensor(prior_list).to(DEVICE) #Ensure conversion of lists into tensors.\n",
    "\n",
    "    # Retrieve desired distribution class based on string.\n",
    "    dist_class_dict = {\n",
    "            'TruncatedNormal': TruncatedNormal,\n",
    "            'RescaledLogitNormal': RescaledLogitNormal,\n",
    "            'MultivariateLogitNormal': MultivariateLogitNormal\n",
    "            }\n",
    "    THETA_PRIOR_CLASS = dist_class_dict[THETA_DIST]\n",
    "    THETA_POST_CLASS = dist_class_dict[THETA_POST_DIST] if THETA_POST_DIST else dist_class_dict[THETA_DIST]\n",
    "    \n",
    "    # Define prior\n",
    "    p_theta = THETA_PRIOR_CLASS(loc = prior_means_tensor, scale = prior_sds_tensor, a = prior_lowers_tensor, b = prior_uppers_tensor)\n",
    "\n",
    "    # Initialize posterior q(theta) using its prior p(theta)\n",
    "    learn_cov = (THETA_POST_DIST == 'MultivariateLogitNormal')\n",
    "    if THETA_POST_INIT is None:\n",
    "        THETA_POST_INIT = PRIOR_DIST_DETAILS_DICT\n",
    "    q_theta = MeanField(DEVICE, param_names, THETA_POST_INIT, THETA_POST_CLASS, learn_cov)\n",
    "\n",
    "    #Record loss throughout training\n",
    "    best_loss = 1e15\n",
    "    losses = []\n",
    "\n",
    "    #Initiate optimizers.\n",
    "    optimizer = optim.Adamax(list(q_theta.parameters()), lr = LR)\n",
    "    \n",
    "    #Training loop\n",
    "    with tqdm(total = NITER, desc = f'Learning SDE and hidden parameters.', position = -1) as tq:\n",
    "        for it in range(1, NITER + 1):\n",
    "            optimizer.zero_grad()                \n",
    "            \n",
    "            # Sample theta ~ q(theta) and compute log q(theta)\n",
    "            theta_dict, theta, log_q_theta, _ = q_theta(BATCH_SIZE)\n",
    "            \n",
    "            # Compute log p(theta)\n",
    "            log_p_theta = p_theta.log_prob(theta).sum(-1)\n",
    "\n",
    "            # Compute log p(x|theta)\n",
    "            log_lik, drift, diffusion_sqrt = calc_log_lik(x, theta_dict, DT, SBM_SDE, X0_PRIOR)\n",
    "            print(log_lik.shape)\n",
    "\n",
    "            # Compute negative ELBO: -(log p(theta) + log p(x|theta) - log q (theta))\n",
    "            loss = -log_p_theta.mean() - log_lik.mean() + log_q_theta.mean()\n",
    "            best_loss = loss if loss < best_loss else best_loss\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            if it % PRINT_EVERY == 0:\n",
    "                print('Iteration {} loss: {}'.format(it, loss))\n",
    "\n",
    "            loss.backward()\n",
    "            #torch.nn.utils.clip_grad_norm_(ELBO_params, 5.0)\n",
    "            optimizer.step()\n",
    "        \n",
    "            if it % DECAY_STEP_SIZE == 0:\n",
    "                optimizer.param_groups[0]['lr'] *= LR_DECAY\n",
    "\n",
    "            tq.update()\n",
    "    \n",
    "    return q_theta, p_theta, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training parameters\n",
    "niter = 2000\n",
    "train_lr = 0.01 #ELBO learning rate\n",
    "batch_size = 40 #3 - number needed to fit UCI HPC3 RAM requirements with 16 GB RAM at t = 5000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35450499415248c284ac77abaaad22e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Learning SDE and hidden parameters.'), FloatProgress(value=0.0, max=5.0), HTML(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 1001, 3])\n",
      "k_* match tensor(True) tensor(True) tensor(True)\n",
      "drift match tensor(True) tensor(True) tensor(True)\n",
      "drift diffusion torch.Size([40, 1000, 3]) torch.Size([40, 1, 3, 3])\n",
      "torch.Size([40])\n",
      "torch.Size([40, 1001, 3])\n",
      "k_* match tensor(True) tensor(True) tensor(True)\n",
      "drift match tensor(True) tensor(True) tensor(True)\n",
      "drift diffusion torch.Size([40, 1000, 3]) torch.Size([40, 1, 3, 3])\n",
      "torch.Size([40])\n",
      "torch.Size([40, 1001, 3])\n",
      "k_* match tensor(True) tensor(True) tensor(True)\n",
      "drift match tensor(True) tensor(True) tensor(True)\n",
      "drift diffusion torch.Size([40, 1000, 3]) torch.Size([40, 1, 3, 3])\n",
      "torch.Size([40])\n",
      "torch.Size([40, 1001, 3])\n",
      "k_* match tensor(True) tensor(True) tensor(True)\n",
      "drift match tensor(True) tensor(True) tensor(True)\n",
      "drift diffusion torch.Size([40, 1000, 3]) torch.Size([40, 1, 3, 3])\n",
      "torch.Size([40])\n",
      "torch.Size([40, 1001, 3])\n",
      "k_* match tensor(True) tensor(True) tensor(True)\n",
      "drift match tensor(True) tensor(True) tensor(True)\n",
      "drift diffusion torch.Size([40, 1000, 3]) torch.Size([40, 1, 3, 3])\n",
      "torch.Size([40])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Call training loop function for SCON-C.\n",
    "q_theta_raw, p_theta, losses = train(\n",
    "        active_device, train_lr, 5, batch_size, x_path, t, dt_flow, n, \n",
    "        t_span_tensor, i_s_tensor, i_d_tensor, temp_tensor, temp_ref,\n",
    "        SBM_SDE_class, diffusion_type, x0_prior_SCON, SCON_C_priors_details,\n",
    "        THETA_DIST=theta_dist, THETA_POST_DIST=theta_post_dist,\n",
    "        LR_DECAY = 1.0, PRINT_EVERY = niter // 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call training loop function for SCON-C.\n",
    "q_theta_raw_mf, p_theta_mf, losses_mf = train(\n",
    "        active_device, train_lr, niter, batch_size, x_path, t, dt_flow, n, \n",
    "        t_span_tensor, i_s_tensor, i_d_tensor, temp_tensor, temp_ref,\n",
    "        SBM_SDE_class, diffusion_type, x0_prior_SCON, SCON_C_priors_details,\n",
    "        THETA_DIST=theta_dist, THETA_POST_DIST=theta_dist,\n",
    "        LR_DECAY = 1.0, PRINT_EVERY = niter // 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(loss_hist_list, labels, time_hist_list=None, xscale='linear', ymin=None, ymax=None):\n",
    "    plt.rcParams.update({'font.size': 12, 'lines.linewidth': 2, 'figure.figsize': (8, 6)})\n",
    "    \n",
    "    for loss_hist, label in zip(loss_hist_list, labels):\n",
    "        plt.plot(loss_hist, label=label)\n",
    "    \n",
    "    plt.xlabel('iteration')\n",
    "    plt.title('Loss v iteration')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.xscale(xscale)\n",
    "    plt.ylim((ymin, ymax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['full-rank', 'mean-field']\n",
    "plot_loss([losses, losses_mf], labels, ymax=-550) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the distribution from a MeanField object\n",
    "def extract_dist(q):\n",
    "    a, b = q.lowers, q.uppers\n",
    "    loc = q.means\n",
    "    if not q.learn_cov:\n",
    "        scale = torch.max(q.sds, torch.ones_like(q.sds) * 1e-8)\n",
    "        #scale = D.transform_to(q.dist.arg_constraints['scale'])(q.sds)\n",
    "        return q.dist(loc, scale=scale, a=a, b=b)\n",
    "    else:\n",
    "        scale = D.transform_to(q.dist.arg_constraints['scale_tril'])(q.sds)\n",
    "        return q.dist(loc, scale_tril=scale, a=a, b=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_theta = extract_dist(q_theta_raw)\n",
    "q_theta_mf = extract_dist(q_theta_raw_mf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_theta(p_theta, q_theta_list, theta_file, labels, param_names, num_pts=1000, eps=1e-5, ncols=4,\n",
    "               colors=None, linestyles=None, device=active_device):\n",
    "    plt.rcParams.update({'font.size': 16, 'lines.linewidth': 2})\n",
    "    \n",
    "    # Load posterior and define plot boundaries\n",
    "    a, b = p_theta.a, p_theta.b\n",
    "    x0 = p_theta.mean - 4*p_theta.stddev\n",
    "    x1 = p_theta.mean + 4*p_theta.stddev\n",
    "    q_marginals = []\n",
    "    for q_theta in q_theta_list:\n",
    "        if isinstance(q_theta, RescaledLogitNormal):\n",
    "            q_marginal = q_theta\n",
    "        else:\n",
    "            scale_post = torch.diag(q_theta.covariance_matrix).sqrt()\n",
    "            q_marginal = RescaledLogitNormal(q_theta.loc, scale_post, a=a, b=b)\n",
    "        q_marginals.append(q_marginal)\n",
    "        x0 = torch.fmin(x0, q_marginal.mean - 4*q_marginal.stddev)\n",
    "        x1 = torch.fmax(x1, q_marginal.mean + 4*q_marginal.stddev)\n",
    "        #print(x0, x1)\n",
    "    x0 = torch.fmax(x0, a).detach()\n",
    "    x1 = torch.fmin(x1, b).detach()\n",
    "    x = torch.from_numpy(np.linspace(x0, x1, num_pts))\n",
    "    \n",
    "    # Load true theta\n",
    "    theta = torch.load(theta_file, map_location=device)\n",
    "    \n",
    "    # Compute pdfs\n",
    "    #print(x[0, :], x[-1, :])\n",
    "    prior_pdf = torch.exp(p_theta.log_prob(x)).detach()\n",
    "    post_pdfs = []\n",
    "    for q_theta in q_marginals:\n",
    "        post_pdf = torch.exp(q_theta.log_prob(x)).detach()\n",
    "        post_pdfs.append(post_pdf)\n",
    "    \n",
    "    # Plot prior v posterior v true theta\n",
    "    num_params = len(param_names)\n",
    "    nrows = int(num_params / ncols) + 1\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 4, nrows * 4))\n",
    "    axes = np.atleast_2d(axes)\n",
    "    k = 0\n",
    "    if colors is None: colors = [cm.tab10(i+1) for i in range(len(post_pdfs))]\n",
    "    if linestyles is None: linestyles = ['-'] * len(post_pdfs)\n",
    "    for i, row in enumerate(axes):\n",
    "        for j, ax in enumerate(row):\n",
    "            if k < num_params:\n",
    "                key = param_names[k]\n",
    "                ax.plot(x[:, k], prior_pdf[:, k], label='Prior $p(\\\\theta)$', color='tab:blue')\n",
    "                for post_pdf, post_dist, c, l in zip(post_pdfs, labels, colors, linestyles):\n",
    "                    label = 'Posterior $q(\\\\theta)$ {}'.format(post_dist)\n",
    "                    ax.plot(x[:, k], post_pdf[:, k], label=label, color=c, linestyle=l)\n",
    "                ax.axvline(theta[key], color='gray', label='True $\\\\theta$')\n",
    "                ax.set_xlabel(key)\n",
    "                if j == 0: ax.set_ylabel('density')\n",
    "            elif k == num_params:\n",
    "                handles, labels = axes[0, 0].get_legend_handles_labels()\n",
    "                ax.legend(handles, labels, loc='center')\n",
    "                ax.axis('off')\n",
    "            else:\n",
    "                fig.delaxes(axes[i, j])\n",
    "            k += 1  \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Marginal distributions')\n",
    "    plt.subplots_adjust(top=0.95)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_theta(p_theta, [q_theta, q_theta_mf], theta_file, labels, q_theta_raw.keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr(q_theta_list, labels, param_names, num_samples=100000):\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "    \n",
    "    # Calculate empirical correlation\n",
    "    corr_list = []\n",
    "    for q_theta in q_theta_list:\n",
    "        assert isinstance(q_theta, MultivariateLogitNormal)\n",
    "        samples = q_theta.sample((num_samples, )) # (N, D)\n",
    "        corr_mc = np.corrcoef(samples.T)\n",
    "        corr_list.append(corr_mc)\n",
    "    \n",
    "    # Plot\n",
    "    num_cols = len(q_theta_list)\n",
    "    fig, axes = plt.subplots(1, num_cols, figsize=(8*num_cols, 8))\n",
    "    axes = np.atleast_1d(axes)\n",
    "    D = len(q_theta_list[0].loc)\n",
    "    \n",
    "    for i, ax in enumerate(axes):\n",
    "        plot = ax.imshow(corr_list[i], cmap='coolwarm', vmin=-1, vmax=1)\n",
    "        ax.set_xticks(range(D))\n",
    "        ax.set_xticklabels(param_names, rotation='vertical')\n",
    "        ax.set_yticks(range(D))\n",
    "        ax.set_yticklabels(param_names)\n",
    "        ax.set_title(labels[i])\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.colorbar(plot, ax=axes, shrink=0.8)\n",
    "    plt.suptitle('Correlation between parameters')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr([q_theta], [None], q_theta_raw.keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with unobserved $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_theta_y_file = '../training_pt_outputs/q_theta_iter_310000_t_1000_dt_1.0_batch_40_layers_5_lr_0.0005_sd_scale_0.333_SCON-C_logit_multi_2021_09_22_11_48_46.pt'\n",
    "q_theta_y_mf_file = '../training_pt_outputs/q_theta_iter_250000_t_1000_dt_1.0_batch_45_layers_5_lr_0.0005_sd_scale_0.333_SCON-C_no_CO2_logit_alt_2021_09_23_07_08_19.pt'\n",
    "device=torch.device('cpu')\n",
    "q_theta_y = extract_dist(torch.load(q_theta_y_file, map_location=device))\n",
    "q_theta_y_mf = extract_dist(torch.load(q_theta_y_mf_file, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_file = '../generated_data/SCON-C_CO2_logit_alt_sample_y_t_1000_dt_0-01_sd_scale_0-333_rsample.pt'\n",
    "labels = ['full-rank, observe $y$', 'full-rank, observe $x$', 'mean-field, observe $y$', 'mean-field observe $x$']\n",
    "plot_theta(p_theta, [q_theta_y, q_theta, q_theta_y_mf, q_theta_mf], theta_file, labels, q_theta_raw.keys,\n",
    "           colors=[cm.tab10(1), cm.tab10(1), cm.tab10(2), cm.tab10(2)], linestyles=['-', '--', '-', '--'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr([q_theta_y, q_theta], ['Observe $y$', 'Observe $x$'], q_theta_raw.keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OMGGG I just realized `SBM_SDE` would need to change as well to support theta-from-multiple-x’s inference. We need to compute the negative ELBO:\n",
    "$$\\mathcal{L} = - \\frac{1}{S} \\sum_s \\left[ \\log p(\\theta^{(s)}) + \\log p(x|\\theta^{(s)}) - \\log q(\\theta^{(s)}) \\right]$$\n",
    "where $S$ is the batch size.\n",
    "\n",
    "With $M$ sequences of $x_i, i = 1, ..., M$, the log likelihood term is now:\n",
    "$$\\log p(x|\\theta^{(s)}) = \\sum_i \\log p(x_i|\\theta^{(s)})$$\n",
    "\n",
    "So drift diffusion needs to take `C_PATH` of size `(S, M, N, D)` (currently `(S, N, D)`) and return a log likelihood tensor of size `(S, M)` (currently `(S, )`), where $N$ is the number of time steps and $D$ is the state dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
