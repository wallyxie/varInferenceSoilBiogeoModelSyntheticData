{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hack to import from a parent directory\n",
    "import sys\n",
    "path = '..'\n",
    "if path not in sys.path:\n",
    "    sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python-related imports\n",
    "import math, sys\n",
    "from typing import Dict, Tuple, Union\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "\n",
    "#Torch-related imports\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch import nn\n",
    "import torch.distributions as D\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#Module imports\n",
    "from SBM_SDE_classes_optim import *\n",
    "from obs_and_flow import *\n",
    "from training import *\n",
    "from plotting import *\n",
    "from mean_field import *\n",
    "from TruncatedNormal import *\n",
    "from LogitNormal import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import time\n",
    "from scipy.optimize import bisect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'data/dt_01_t_100000_minibatch_logit_theta_trunc_trans/SCON_C_{}.pt'\n",
    "target_hyperparams_file = save_dir.format('target_hyperparams')\n",
    "hyperparams_file = save_dir.format('hyperparams')\n",
    "theta_file = save_dir.format('theta')\n",
    "x_file = save_dir.format('x')\n",
    "x0_dist_file = save_dir.format('x0_dist')\n",
    "y_dict_file = save_dir.format('y_dict')\n",
    "\n",
    "target_hyperparams = torch.load(target_hyperparams_file)\n",
    "theta_hyperparams = torch.load(hyperparams_file)\n",
    "theta_samples = torch.load(theta_file)\n",
    "x = torch.load(x_file)\n",
    "p_x0 = torch.load(x0_dist_file)\n",
    "y_dict = torch.load(y_dict_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'u_M': tensor([-6.4362,  0.3104,  0.0000,  1.0000]),\n",
       "  'a_SD': tensor([0.0000, 0.7465, 0.0000, 1.0000]),\n",
       "  'a_DS': tensor([0.0000, 0.7465, 0.0000, 1.0000]),\n",
       "  'a_M': tensor([0.0000, 0.7465, 0.0000, 1.0000]),\n",
       "  'a_MSC': tensor([0.0000, 0.7465, 0.0000, 1.0000]),\n",
       "  'k_S_ref': tensor([-7.6004,  0.3100,  0.0000,  1.0000]),\n",
       "  'k_D_ref': tensor([-7.1301,  0.3101,  0.0000,  1.0000]),\n",
       "  'k_M_ref': tensor([-7.2637,  0.3101,  0.0000,  1.0000]),\n",
       "  'Ea_S': tensor([  0.0000,   0.9685,  10.0000, 100.0000]),\n",
       "  'Ea_D': tensor([ -0.3137,   0.8252,  10.0000, 100.0000]),\n",
       "  'Ea_M': tensor([ -0.3137,   0.8252,  10.0000, 100.0000]),\n",
       "  'c_SOC': tensor([0.0000, 0.7465, 0.0000, 1.0000]),\n",
       "  'c_DOC': tensor([-4.5951,  0.3137,  0.0000,  1.0000]),\n",
       "  'c_MBC': tensor([-4.5951,  0.3137,  0.0000,  1.0000])},\n",
       " {'u_M': tensor(0.0026),\n",
       "  'a_SD': tensor(0.4455),\n",
       "  'a_DS': tensor(0.1643),\n",
       "  'a_M': tensor(0.6045),\n",
       "  'a_MSC': tensor(0.3080),\n",
       "  'k_S_ref': tensor(0.0003),\n",
       "  'k_D_ref': tensor(0.0009),\n",
       "  'k_M_ref': tensor(0.0009),\n",
       "  'Ea_S': tensor(39.9311),\n",
       "  'Ea_D': tensor(40.9399),\n",
       "  'Ea_M': tensor(37.7866),\n",
       "  'c_SOC': tensor(0.5339),\n",
       "  'c_DOC': tensor(0.0077),\n",
       "  'c_MBC': tensor(0.0141)})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_hyperparams, theta_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'u_M': tensor([1.6000e-03, 5.3280e-04, 0.0000e+00, 1.0000e+00]),\n",
       " 'a_SD': tensor([0.5000, 0.1665, 0.0000, 1.0000]),\n",
       " 'a_DS': tensor([0.5000, 0.1665, 0.0000, 1.0000]),\n",
       " 'a_M': tensor([0.5000, 0.1665, 0.0000, 1.0000]),\n",
       " 'a_MSC': tensor([0.5000, 0.1665, 0.0000, 1.0000]),\n",
       " 'k_S_ref': tensor([5.0000e-04, 1.6650e-04, 0.0000e+00, 1.0000e+00]),\n",
       " 'k_D_ref': tensor([8.0000e-04, 2.6640e-04, 0.0000e+00, 1.0000e+00]),\n",
       " 'k_M_ref': tensor([7.0000e-04, 2.3310e-04, 0.0000e+00, 1.0000e+00]),\n",
       " 'Ea_S': tensor([ 55.0000,  18.3150,  10.0000, 100.0000]),\n",
       " 'Ea_D': tensor([ 48.0000,  15.9840,  10.0000, 100.0000]),\n",
       " 'Ea_M': tensor([ 48.0000,  15.9840,  10.0000, 100.0000]),\n",
       " 'c_SOC': tensor([0.5000, 0.1665, 0.0000, 1.0000]),\n",
       " 'c_DOC': tensor([0.0100, 0.0033, 0.0000, 1.0000]),\n",
       " 'c_MBC': tensor([0.0100, 0.0033, 0.0000, 1.0000])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $x$ be the observed states (i.e. SOC, DOC, and MBC in the SCON-SS model) and $\\theta$ be the model parameters. The generative model is:\n",
    "- $\\theta \\sim \\text{LogitNormal}(\\mu_\\theta, \\sigma_\\theta, a, b)$\n",
    "- $x_1 \\sim p(x_1|\\theta)$\n",
    "- For $i=2, ..., T$: $x_i \\sim p(x_i|x_{i-1}, \\theta)$\n",
    "\n",
    "We use variational posterior $q_\\phi(\\theta)=\\text{MultivariateLogitNormal}(\\hat{\\mu}, \\hat{L}\\hat{L}^T, a, b)$ with variational parameter $\\phi=(\\hat{\\mu}, \\hat{L})$. For minibatching, we partition the sequence $x$ into $B$ consecutive subsequences $x_b$. We sample a minibatch by drawing $b$ uniformly from $1, ..., B$. Let $u_b$ and $v_b$ denote the endpoints of the $b$th subsequence, $x_b = x_{u_b:v_b}$. The minibatch loss is:\n",
    "$$\n",
    "\\mathcal{L} = E_{q_\\phi(\\theta)}\\left[ \\log p(\\theta) - \\log q(\\theta) + \\log p(x_b|\\theta) \\right] \\\\\n",
    "$$\n",
    "\n",
    "where the log likelihood term is given by:\n",
    "$$\n",
    "\\log p(x_b|\\theta) = \\sum_{i=u_b}^{v_b} \\log p(x_i|x_{i-1}, \\theta)\n",
    "$$\n",
    "if $u_b > 1$, otherwise: $\\log p(x_b|\\theta) = \\log p(x_1|\\theta) + \\sum_{i=u_b+1}^{v_b} \\log p(x_i|x_{i-1}, \\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reparameterized gradient wrt variational parameter $\\phi$:\n",
    "$$\n",
    "\\nabla_\\phi \\mathcal{L} \\approx -\\frac{1}{S} \\sum_s \\nabla_\\phi \\left[\n",
    "    \\log p(g_\\phi(\\epsilon^{(s)})) + \\log p(x_b|g_\\phi(\\epsilon^{(s)})) - \\log q(g_\\phi(\\epsilon^{(s)}))\n",
    "    \\right]\n",
    "$$\n",
    "\n",
    "where $\\epsilon^{(s)} \\sim \\text{MultivariateNormal}(0, I)$ and $g_\\phi(\\epsilon)=(b-a) \\odot \\sigma(\\hat{\\mu}+\\hat{L}\\epsilon)+a$, with $\\odot$ to denote elementwise multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudocode:\n",
    "- for each iteration $n=1, ..., N$:\n",
    "  - Sample minibatch: $b \\sim \\text{Uniform}(1, ..., B)$\n",
    "  - Sample $\\theta$: $\\theta^{(s)} \\sim q(\\theta)$ for $s=1, ..., S$\n",
    "  - `loss` $= -\\frac{1}{S} \\sum_s \\left[\\log p(\\theta^{(s)}) + \\log p(x_b|\\theta^{(s)}) - \\log q(\\theta^{(s)})\\right]$ \n",
    "  - `loss.backward()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation note:** The covariance matrix $\\Sigma$ of the multivariate normal in PyTorch is parameterized in terms of a lower-triangular matrix $L$ with positive-valued diagonal entries, such that $\\Sigma = LL^T$. In practice, we optimize the unconstrained transformation of $L$. The following code shows how the mapping is done back and forth:\n",
    "```\n",
    "def to_constrained(self, x):\n",
    "    # Takes unconstrained square matrix x and returns L, s.t. Sigma = LL^T\n",
    "    return x.tril(-1) + x.diagonal(dim1=-2, dim2=-1).exp().diag_embed()\n",
    "\n",
    "def to_unconstrained(self, y):\n",
    "    return y.tril(-1) + y.diagonal(dim1=-2, dim2=-1).log().diag_embed()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = int(y_dict['t_x'][-1])\n",
    "dt_flow = 1.0 # [1.0, 0.5]\n",
    "temp_ref = 283\n",
    "temp_rise = 5 #High estimate of 5 celsius temperature rise by 2100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "torch.set_printoptions(precision = 8)\n",
    "\n",
    "#Neural SDE parameters\n",
    "n = int(t / dt_flow) + 1\n",
    "t_span = np.linspace(0, t, n)\n",
    "t_span_tensor = torch.reshape(torch.Tensor(t_span), [1, n, 1]).to(active_device) #T_span needs to be converted to tensor object. Additionally, facilitates conversion of I_S and I_D to tensor objects.\n",
    "\n",
    "#Specify desired SBM SDE model type and details.\n",
    "SBM_SDE_class = 'SCON'\n",
    "diffusion_type = 'C'\n",
    "theta_dist = 'RescaledLogitNormal' #String needs to be exact name of the distribution class. Other option is 'RescaledLogitNormal'.\n",
    "theta_post_dist = 'MultivariateLogitNormal'\n",
    "\n",
    "#Generate exogenous input vectors.\n",
    "#Obtain temperature forcing function.\n",
    "temp_tensor = temp_gen(t_span_tensor, temp_ref, temp_rise).to(active_device)\n",
    "\n",
    "#Obtain SOC and DOC pool litter input vectors for use in flow SDE functions.\n",
    "i_s_tensor = i_s(t_span_tensor).to(active_device) #Exogenous SOC input function\n",
    "i_d_tensor = i_d(t_span_tensor).to(active_device) #Exogenous DOC input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_log_lik(C_PATH, PARAMS_DICT, DT, SBM_SDE_CLASS, INIT_PRIOR, start_idx, end_idx, calc_full=False):      \n",
    "    # Compute drift and diffusion\n",
    "    if calc_full:\n",
    "        drift, diffusion_sqrt = SBM_SDE_CLASS.drift_diffusion(C_PATH, PARAMS_DICT, diffusion_matrix=False)\n",
    "        #p_x = D.multivariate_normal.MultivariateNormal(loc = C_PATH[:, :-1, :] + drift * DT,\n",
    "        #                                               scale_tril = diffusion_sqrt * math.sqrt(DT)) \n",
    "        p_x = TruncatedNormal(loc = C_PATH[:, :-1, :] + drift * DT,\n",
    "                              scale = diffusion_sqrt * math.sqrt(DT), a = 0, b = 1000)\n",
    "    \n",
    "        # Compute log p(x|theta) = log p(x|x0, theta) + log p(x0|theta)\n",
    "        log_p_x = p_x.log_prob(C_PATH[:, 1:, :]).sum(-1) # log p(x_i|x_{i-1}, theta), (batch_size, N - 1)\n",
    "        log_p_x0 = INIT_PRIOR.log_prob(C_PATH[:, 0, :]).sum(-1) # log p(x0|theta), (batch_size, )\n",
    "        ll_minibatch = log_p_x[:, start_idx:end_idx-1].sum(-1) # (batch_size, )\n",
    "        if start_idx == 0:\n",
    "            ll_minibatch += log_p_x0\n",
    "        ll_full = log_p_x.sum(-1) + log_p_x0 # log p(x|theta), (batch_size, )\n",
    "    \n",
    "    else:\n",
    "        drift, diffusion_sqrt = SBM_SDE_CLASS.drift_diffusion(C_PATH[:, start_idx:end_idx-1, :], PARAMS_DICT, start_idx, end_idx, diffusion_matrix=False) \n",
    "        p_x = TruncatedNormal(loc = C_PATH[:, start_idx:end_idx-1, :] + drift * DT,\n",
    "                              scale = diffusion_sqrt * math.sqrt(DT), a = 0, b = 1000)\n",
    "    \n",
    "        # Compute log p(x|theta) = log p(x|x0, theta) + log p(x0|theta)\n",
    "        ll_minibatch = p_x.log_prob(C_PATH[:, start_idx+1:end_idx, :]).sum((-2, -1)) # log p(x|x0, theta), (batch_size, )\n",
    "        if start_idx == 0:\n",
    "            ll_minibatch += INIT_PRIOR.log_prob(C_PATH[:, 0, :]).sum(-1) # log p(x0|theta), (batch_size, )\n",
    "        ll_full = None\n",
    "    \n",
    "    return ll_minibatch, ll_full\n",
    "\n",
    "def train(DEVICE, LR, NITER, BATCH_SIZE, MINIBATCH_SIZE, X_ALL, T, DT, N,\n",
    "          T_SPAN_TENSOR, I_S_TENSOR, I_D_TENSOR, TEMP_TENSOR, TEMP_REF,\n",
    "          SBM_SDE_CLASS, DIFFUSION_TYPE, X0_PRIOR, PRIOR_DIST_DETAILS_DICT, \n",
    "          THETA_DIST = None, THETA_POST_DIST = None, THETA_POST_INIT = None,\n",
    "          LR_DECAY = 0.8, DECAY_STEP_SIZE = 50000, PRINT_EVERY = 100, MINIBATCH_INDICES=None,\n",
    "          CALC_LOSS_EVERY=100):\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Instantiate SBM_SDE object based on specified model and diffusion type.\n",
    "    SBM_SDE_class_dict = {\n",
    "            'SCON': SCON_optim,\n",
    "            'SAWB': SAWB_optim,\n",
    "            'SAWB-ECA': SAWB_ECA_optim\n",
    "            }\n",
    "    if SBM_SDE_CLASS not in SBM_SDE_class_dict:\n",
    "        raise NotImplementedError('Other SBM SDEs aside from SCON, SAWB, and SAWB-ECA have not been implemented yet.')\n",
    "    SBM_SDE_class = SBM_SDE_class_dict[SBM_SDE_CLASS]\n",
    "    SBM_SDE = SBM_SDE_class(T_SPAN_TENSOR, I_S_TENSOR, I_D_TENSOR, TEMP_TENSOR, TEMP_REF, DIFFUSION_TYPE)\n",
    "\n",
    "    # If dt_flow > dt, we need to throw away additional points from x,\n",
    "    # then add a leading dimension since SBM_SDE_classes assumes that first dim is batch size\n",
    "    # x.shape: (N_gen, state_dim) -> (1, N_flow, state_dim)\n",
    "    step = (X_ALL.shape[0] - 1) // (N - 1)\n",
    "    x = X_ALL[::step, :SBM_SDE.state_dim].unsqueeze(0)\n",
    "    assert x.shape == (1, N, SBM_SDE.state_dim)\n",
    "\n",
    "    # Convert prior details dictionary values to tensors.\n",
    "    param_names = list(PRIOR_DIST_DETAILS_DICT.keys())\n",
    "    prior_list = list(zip(*(PRIOR_DIST_DETAILS_DICT[k] for k in param_names))) #Unzip prior distribution details from dictionary values into individual lists.\n",
    "    prior_means_tensor, prior_sds_tensor, prior_lowers_tensor, prior_uppers_tensor = torch.tensor(prior_list).to(DEVICE) #Ensure conversion of lists into tensors.\n",
    "\n",
    "    # Retrieve desired distribution class based on string.\n",
    "    dist_class_dict = {\n",
    "            'TruncatedNormal': TruncatedNormal,\n",
    "            'RescaledLogitNormal': RescaledLogitNormal,\n",
    "            'MultivariateLogitNormal': MultivariateLogitNormal\n",
    "            }\n",
    "    THETA_PRIOR_CLASS = dist_class_dict[THETA_DIST]\n",
    "    THETA_POST_CLASS = dist_class_dict[THETA_POST_DIST] if THETA_POST_DIST else dist_class_dict[THETA_DIST]\n",
    "    \n",
    "    # Define prior\n",
    "    p_theta = THETA_PRIOR_CLASS(loc = prior_means_tensor, scale = prior_sds_tensor, a = prior_lowers_tensor, b = prior_uppers_tensor)\n",
    "\n",
    "    # Initialize posterior q(theta) using its prior p(theta)\n",
    "    learn_cov = (THETA_POST_DIST == 'MultivariateLogitNormal')\n",
    "    if THETA_POST_INIT is None:\n",
    "        THETA_POST_INIT = PRIOR_DIST_DETAILS_DICT\n",
    "    q_theta = MeanField(DEVICE, param_names, THETA_POST_INIT, THETA_POST_CLASS, learn_cov)\n",
    "\n",
    "    #Record loss throughout training (needs moving average bc of minibatching)\n",
    "    #best_loss = 1e15\n",
    "    losses = []\n",
    "    losses_full = []\n",
    "    times = []\n",
    "\n",
    "    #Initiate optimizers.\n",
    "    optimizer = optim.Adamax(list(q_theta.parameters()), lr = LR)\n",
    "    \n",
    "    # Sample minibatch endpoints\n",
    "    if MINIBATCH_SIZE < N:\n",
    "        if MINIBATCH_INDICES is None:\n",
    "            step = (N - MINIBATCH_SIZE) // NITER\n",
    "            MINIBATCH_INDICES = torch.arange(0, (N - MINIBATCH_SIZE), step)\n",
    "        rand = torch.randint(len(MINIBATCH_INDICES), (NITER, ))\n",
    "        print(torch.min(torch.bincount(rand)))\n",
    "        batch_indices = MINIBATCH_INDICES[rand]\n",
    "    \n",
    "    #Training loop\n",
    "    t0 = time.time()\n",
    "    with tqdm(total = NITER, desc = f'Learning SDE and hidden parameters.', position = -1) as tq:\n",
    "        for it in range(1, NITER + 1):\n",
    "            optimizer.zero_grad()    \n",
    "            \n",
    "            # Sample theta ~ q(theta) and compute log q(theta)\n",
    "            theta_dict, theta, log_q_theta, _ = q_theta(BATCH_SIZE)\n",
    "            \n",
    "            # Compute log p(theta)\n",
    "            log_p_theta = p_theta.log_prob(theta).sum(-1)\n",
    "\n",
    "            # Compute log p(x_{u-1:v}|theta) (unless u = 0, then x_{u:v})\n",
    "            if MINIBATCH_SIZE < N:\n",
    "                start_idx = max(0, batch_indices[it-1] - 1)              # u-1 if u > 0, else 0\n",
    "                end_idx = min(N, batch_indices[it-1] + MINIBATCH_SIZE)   # v\n",
    "            else:\n",
    "                start_idx, end_idx = 0, N\n",
    "            #print(batch_id, start_idx, end_idx)\n",
    "            calc_loss = (it % CALC_LOSS_EVERY == 0) or (it == 1)\n",
    "            ll, ll_full = calc_log_lik(x, theta_dict, DT, SBM_SDE, X0_PRIOR,\n",
    "                                       start_idx, end_idx, calc_loss)\n",
    "\n",
    "            # Compute negative ELBO: -(log p(theta) + log p(x|theta) - log q(theta))\n",
    "            loss = -log_p_theta.mean() - N/MINIBATCH_SIZE * ll.mean() + log_q_theta.mean()\n",
    "            losses.append(loss.item())\n",
    "            if calc_loss:\n",
    "                loss_full = -log_p_theta.mean() - ll_full.mean() + log_q_theta.mean()\n",
    "                losses_full.append(loss_full.item())\n",
    "\n",
    "            # Take a gradient step\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(q_theta.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Record time\n",
    "            times.append(time.time() - t0)\n",
    "            \n",
    "            if it % PRINT_EVERY == 0:\n",
    "                if calc_loss:\n",
    "                    print('Iteration {} loss: {}'.format(it, loss_full))\n",
    "                else:\n",
    "                    k = 1000\n",
    "                    ma_loss = losses[it-k-1:it-1].mean()\n",
    "                    print('Iteration {} moving average loss: {}'.format(it, ma_loss))\n",
    "        \n",
    "            if it % DECAY_STEP_SIZE == 0:\n",
    "                optimizer.param_groups[0]['lr'] *= LR_DECAY\n",
    "\n",
    "            tq.update()\n",
    "    \n",
    "    return q_theta, p_theta, losses, losses_full, times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training parameters\n",
    "niter = 200000 # niter = epochs * (n - 1) / minibatch_size\n",
    "train_lr = 0.01 #ELBO learning rate\n",
    "batch_size = 40 #3 - number needed to fit UCI HPC3 RAM requirements with 16 GB RAM at t = 5000.\n",
    "minibatch_size = 1000\n",
    "minibatch_indices = torch.arange(0, t + 1 - minibatch_size, minibatch_size) + 1\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([     0,   1000,   2000,   3000,   4000,   5000,   6000,   7000,   8000,\n",
       "          9000,  10000,  11000,  12000,  13000,  14000,  15000,  16000,  17000,\n",
       "         18000,  19000,  20000,  21000,  22000,  23000,  24000,  25000,  26000,\n",
       "         27000,  28000,  29000,  30000,  31000,  32000,  33000,  34000,  35000,\n",
       "         36000,  37000,  38000,  39000,  40000,  41000,  42000,  43000,  44000,\n",
       "         45000,  46000,  47000,  48000,  49000,  50000,  51000,  52000,  53000,\n",
       "         54000,  55000,  56000,  57000,  58000,  59000,  60000,  61000,  62000,\n",
       "         63000,  64000,  65000,  66000,  67000,  68000,  69000,  70000,  71000,\n",
       "         72000,  73000,  74000,  75000,  76000,  77000,  78000,  79000,  80000,\n",
       "         81000,  82000,  83000,  84000,  85000,  86000,  87000,  88000,  89000,\n",
       "         90000,  91000,  92000,  93000,  94000,  95000,  96000,  97000,  98000,\n",
       "         99000, 100000])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, n, minibatch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100001"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, 1, minibatch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Call training loop function for SCON-C.\n",
    "t0 = time.time()\n",
    "q_theta_raw, p_theta, losses_noisy, losses, times = train(\n",
    "        active_device, train_lr, niter, batch_size, minibatch_size, x, t, dt_flow, n, \n",
    "        t_span_tensor, i_s_tensor, i_d_tensor, temp_tensor, temp_ref,\n",
    "        SBM_SDE_class, diffusion_type, p_x0, theta_hyperparams,\n",
    "        THETA_DIST=theta_dist, THETA_POST_DIST = theta_post_dist,\n",
    "        LR_DECAY = 1.0, PRINT_EVERY = max(1, niter // 10), MINIBATCH_INDICES=minibatch_indices)\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call training loop function for SCON-C.\n",
    "t0 = time.time()\n",
    "q_theta_raw_mf, p_theta_mf, losses_noisy_mf, losses_mf, times_mf = train(\n",
    "        active_device, train_lr, niter, batch_size, minibatch_size, x, t, dt_flow, n, \n",
    "        t_span_tensor, i_s_tensor, i_d_tensor, temp_tensor, temp_ref,\n",
    "        SBM_SDE_class, diffusion_type, p_x0, theta_hyperparams, THETA_DIST=theta_dist, \n",
    "        LR_DECAY = 1.0, PRINT_EVERY = max(1, niter // 10), MINIBATCH_INDICES=minibatch_indices)\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call training loop function for SCON-C.\n",
    "t0 = time.time()\n",
    "q_theta_raw_trunc_mf, p_theta_trunc_mf, losses_noisy_trunc_mf, losses_trunc_mf, times_trunc_mf = train(\n",
    "        active_device, 1e-4, niter, batch_size, minibatch_size, x, t, dt_flow, n, \n",
    "        t_span_tensor, i_s_tensor, i_d_tensor, temp_tensor, temp_ref,\n",
    "        SBM_SDE_class, diffusion_type, p_x0, theta_hyperparams,\n",
    "        THETA_DIST=theta_dist, THETA_POST_DIST='TruncatedNormal', THETA_POST_INIT=target_hyperparams,\n",
    "        LR_DECAY = 1.0, PRINT_EVERY = max(1, niter // 10), MINIBATCH_INDICES=minibatch_indices)\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(loss_hist_list, labels, xscale='linear', ymin=None, ymax=None,\n",
    "              colors=None, linestyles=None, xvals=None, xlabel='iteration'):\n",
    "    plt.rcParams.update({'font.size': 12, 'lines.linewidth': 2, 'figure.figsize': (8, 6)})\n",
    "    if colors is None:\n",
    "        colors = [cm.tab10(i+1) for i in range(len(labels))]\n",
    "    if linestyles is None:\n",
    "        linestyles = ['-'] * len(labels)\n",
    "    if xvals is None:\n",
    "        xvals = [None] * len(labels)\n",
    "    \n",
    "    for loss_hist, x, label, c, s in zip(loss_hist_list, xvals, labels, colors, linestyles):\n",
    "        if x is None:\n",
    "            plt.plot(loss_hist, label=label, color=c, linestyle=s)\n",
    "        else:\n",
    "            plt.plot(x, loss_hist, label=label, color=c, linestyle=s)\n",
    "    \n",
    "    plt.xlabel(xlabel)\n",
    "    #plt.title('Loss v iteration')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.xscale(xscale)\n",
    "    plt.ylim((ymin, ymax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['full-rank logit normal', 'mean-field logit normal', 'mean-field truncated normal']\n",
    "plot_loss([losses, losses_mf, losses_trunc_mf], labels, ymax=None, ymin=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch = iter * minibatch_size / (n - 1)\n",
    "epochs = [torch.arange(0, niter + 1, 100) * minibatch_size / (n - 1)] * 3\n",
    "plot_loss([losses, losses_mf, losses_trunc_mf], labels, ymax=-62500, ymin=-66000, xvals=epochs, xlabel='epoch', xscale='symlog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_indices = torch.cat((torch.tensor([0]), torch.arange(99, niter + 1, 100)))\n",
    "time_vals = [torch.tensor(times)[time_indices], torch.tensor(times_mf)[time_indices], torch.tensor(times_trunc_mf)[time_indices]]\n",
    "plot_loss([losses, losses_mf, losses_trunc_mf], labels, ymax=-62500, ymin=-66000, xvals=time_vals, xlabel='time', xscale='symlog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['full-rank (full sequence)', 'mean-field (full sequence)',\n",
    "          'full-rank (minibatch size = 5,000)', 'mean-field (minibatch size = 5,000)',\n",
    "          'full-rank (minibatch size = 1,000)', 'mean-field (minibatch size = 1,000)']\n",
    "colors =  [cm.tab10(i) for i in [1, 2, 1, 2, 1, 2]]\n",
    "linestyles = ['-', '-', '--', '--', ':', ':']\n",
    "\n",
    "epochs = [None] * 2 + \\\n",
    "         [torch.arange(0, niter + 1, 100) * size5k / (n - 1)] * 2 + \\\n",
    "         [torch.arange(0, niter + 1, 100) * size1k / (n - 1)] * 2\n",
    "plot_loss([losses_full, losses_mf_full, losses_mini5k, losses_mf_mini5k, losses_mini1k, losses_mf_mini1k],\n",
    "          labels, ymax=-61000, ymin=-62200, \n",
    "          colors=colors, linestyles=linestyles, xvals=epochs, xscale='symlog', xlabel='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_indices = torch.cat((torch.tensor([0]), torch.arange(99, niter + 1, 100) ))\n",
    "time_vals = [times_full, times_mf_full] + [torch.tensor(times)[time_indices] for times in [times_mini5k, times_mf_mini5k, times_mini1k, times_mf_mini1k]]\n",
    "\n",
    "plot_loss([losses_full, losses_mf_full, losses_mini5k, losses_mf_mini5k, losses_mini1k, losses_mf_mini1k],\n",
    "          labels, ymax=-61000, ymin=-62200, xlabel='time', xscale='symlog',\n",
    "          colors=colors, linestyles=linestyles, xvals=time_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['full-rank (minibatch overlap)', 'mean-field (minibatch overlap)',\n",
    "          'full-rank (minibatch no overlap)', 'mean-field (minibatch no overlap)']\n",
    "colors =  [cm.tab10(i) for i in [1, 2, 1, 2]]\n",
    "linestyles = ['-', '-', '--', '--']\n",
    "\n",
    "xvals = [torch.arange(0, niter + 1, 100)] * 4\n",
    "plot_loss([losses_mini5k_over, losses_mf_mini5k_over, losses_mini5k, losses_mf_mini5k],\n",
    "          labels, ymax=-61000, ymin=-62200,\n",
    "          colors=colors, linestyles=linestyles, xvals=xvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the distribution from a MeanField object\n",
    "def extract_dist(q):\n",
    "    a, b = q.lowers, q.uppers\n",
    "    loc = q.means\n",
    "    if not q.learn_cov:\n",
    "        scale = torch.max(q.sds, torch.ones_like(q.sds) * 1e-8)\n",
    "        #scale = D.transform_to(q.dist.arg_constraints['scale'])(q.sds)\n",
    "        return q.dist(loc, scale=scale, a=a, b=b)\n",
    "    else:\n",
    "        scale = D.transform_to(q.dist.arg_constraints['scale_tril'])(q.sds)\n",
    "        return q.dist(loc, scale_tril=scale, a=a, b=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_theta = extract_dist(q_theta_raw)\n",
    "q_theta_mf = extract_dist(q_theta_raw_mf)\n",
    "#q_theta_mini1k = extract_dist(q_theta_raw_mini1k)\n",
    "#q_theta_mf_mini1k = extract_dist(q_theta_raw_mf_mini1k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_theta_trunc_mf = extract_dist(q_theta_raw_trunc_mf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_theta(p_theta, q_theta_list, theta, labels, param_names, num_pts=1000, eps=1e-5, ncols=4,\n",
    "               colors=None, linestyles=None, device=active_device):\n",
    "    plt.rcParams.update({'font.size': 16, 'lines.linewidth': 2})\n",
    "    \n",
    "    # Load posterior and define plot boundaries\n",
    "    if isinstance(p_theta, TruncatedNormal):\n",
    "        a, b = p_theta._from_std_rv(p_theta.a), p_theta._from_std_rv(p_theta.b)\n",
    "    else:\n",
    "        a, b = p_theta.a, p_theta.b\n",
    "    x0 = p_theta.mean - 4*p_theta.stddev\n",
    "    x1 = p_theta.mean + 4*p_theta.stddev\n",
    "    q_marginals = []\n",
    "    for q_theta in q_theta_list:\n",
    "        if isinstance(q_theta, MultivariateLogitNormal):\n",
    "            scale_post = torch.diag(q_theta.covariance_matrix).sqrt()\n",
    "            q_marginal = RescaledLogitNormal(q_theta.loc, scale_post, a=a, b=b)\n",
    "        else:\n",
    "            q_marginal = q_theta\n",
    "        q_marginals.append(q_marginal)\n",
    "        x0 = torch.fmin(x0, q_marginal.mean - 4*q_marginal.stddev)\n",
    "        x1 = torch.fmax(x1, q_marginal.mean + 4*q_marginal.stddev)\n",
    "        #print(x0, x1)\n",
    "    x0 = torch.fmax(x0, a).detach()\n",
    "    x1 = torch.fmin(x1, b).detach()\n",
    "    x = torch.from_numpy(np.linspace(x0, x1, num_pts))\n",
    "    \n",
    "    # Load true theta\n",
    "    #theta = torch.load(theta_file, map_location=device)\n",
    "    \n",
    "    # Compute pdfs\n",
    "    prior_pdf = torch.exp(p_theta.log_prob(x)).detach()\n",
    "    post_pdfs = []\n",
    "    for q_theta in q_marginals:\n",
    "        post_pdf = torch.exp(q_theta.log_prob(x)).detach()\n",
    "        post_pdfs.append(post_pdf)\n",
    "    \n",
    "    # Plot prior v posterior v true theta\n",
    "    num_params = len(param_names)\n",
    "    nrows = int(num_params / ncols) + 1\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 4, nrows * 4))\n",
    "    axes = np.atleast_2d(axes)\n",
    "    k = 0\n",
    "    if colors is None: colors = [cm.tab10(i+1) for i in range(len(post_pdfs))]\n",
    "    if linestyles is None: linestyles = ['-'] * len(post_pdfs)\n",
    "    for i, row in enumerate(axes):\n",
    "        for j, ax in enumerate(row):\n",
    "            if k < num_params:\n",
    "                key = param_names[k]\n",
    "                ax.plot(x[:, k], prior_pdf[:, k], label='Prior', color='tab:blue')\n",
    "                for post_pdf, post_dist, c, l in zip(post_pdfs, labels, colors, linestyles):\n",
    "                    label = 'Posterior {}'.format(post_dist)\n",
    "                    ax.plot(x[:, k], post_pdf[:, k], label=label, color=c, linestyle=l)\n",
    "                ax.axvline(theta[key], color='gray', label='True $\\\\theta$')\n",
    "                ax.set_xlabel(key)\n",
    "                if j == 0: ax.set_ylabel('density')\n",
    "            elif k == num_params:\n",
    "                handles, labels = axes[0, 0].get_legend_handles_labels()\n",
    "                ax.legend(handles, labels, loc='center')\n",
    "                ax.axis('off')\n",
    "            else:\n",
    "                fig.delaxes(axes[i, j])\n",
    "            k += 1  \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Marginal distributions')\n",
    "    plt.subplots_adjust(top=0.95)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['mean-field (size = 1,000)']\n",
    "plot_theta(p_theta_mini, [q_theta_mini],\n",
    "           theta_samples, labels, q_theta_raw_mini.keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_theta(p_theta, [q_theta, q_theta_mf, q_theta_trunc_mf],\n",
    "           theta_samples, labels, q_theta_raw.keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr(q_theta_list, labels, param_names, num_samples=100000):\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "    \n",
    "    # Calculate empirical correlation\n",
    "    corr_list = []\n",
    "    for q_theta in q_theta_list:\n",
    "        assert isinstance(q_theta, MultivariateLogitNormal)\n",
    "        samples = q_theta.sample((num_samples, )) # (N, D)\n",
    "        corr_mc = np.corrcoef(samples.T)\n",
    "        corr_list.append(corr_mc)\n",
    "    \n",
    "    # Plot\n",
    "    num_cols = len(q_theta_list)\n",
    "    fig, axes = plt.subplots(1, num_cols, figsize=(8*num_cols, 8))\n",
    "    axes = np.atleast_1d(axes)\n",
    "    D = len(q_theta_list[0].loc)\n",
    "    \n",
    "    for i, ax in enumerate(axes):\n",
    "        plot = ax.imshow(corr_list[i], cmap='coolwarm', vmin=-1, vmax=1)\n",
    "        ax.set_xticks(range(D))\n",
    "        ax.set_xticklabels(param_names, rotation='vertical')\n",
    "        ax.set_yticks(range(D))\n",
    "        ax.set_yticklabels(param_names)\n",
    "        ax.set_title(labels[i])\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.colorbar(plot, ax=axes, shrink=0.8)\n",
    "    plt.suptitle('Correlation between parameters')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_corr([q_theta_full, q_theta_mini5k, q_theta_mini1k],\n",
    "          ['full', 'minibatch size = 5,000', 'minibatch size = 1,000'], priors.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr([q_theta],\n",
    "          ['minibatch size = 1,000'], q_theta_raw.keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_x(x, t_span, n):\n",
    "    num_sequences, time_steps, state_dim = x.shape\n",
    "    fig, axes = plt.subplots(state_dim, figsize=(15, 15))\n",
    "    step = (x.shape[1] - 1) // (n - 1)\n",
    "    x_plot = x[:, ::step, :state_dim]\n",
    "    #print(x.shape, x_plot.shape)\n",
    "    \n",
    "    labels = ['SOC', 'DOC', 'MBC', 'EEC']\n",
    "    for i, ax in enumerate(axes):\n",
    "        for j in range(num_sequences):\n",
    "            ax.plot(t_span, x_plot[j, :, i])\n",
    "        ax.set_ylabel(labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_x(x, t_span, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams_file = 'data/dt_1_t_1000000_n_1_minibatch/theta_from_x_hyperparams_scon_c_{}.pt'.format(seed)\n",
    "theta_file = 'data/dt_1_t_1000000_n_1_minibatch/theta_from_x_theta_scon_c_{}.pt'.format(seed)\n",
    "x_file = 'data/dt_1_t_1000000_n_1_minibatch/theta_from_x_x_scon_c_{}.pt'.format(seed)\n",
    "loss_file = 'data/dt_1_t_1000000_n_1_minibatch/theta_from_x_loss_scon_c_{}.pt'.format(seed)\n",
    "q_file = 'data/dt_1_t_1000000_n_1_minibatch/theta_from_x_q_scon_c_{}.pt'.format(seed)\n",
    "time_file = 'data/dt_1_t_1000000_n_1_minibatch/theta_from_x_time_scon_c_{}.pt'.format(seed)\n",
    "torch.save(theta_hyperparams, hyperparams_file)\n",
    "torch.save(theta_samples, theta_file)\n",
    "torch.save(x, x_file)\n",
    "torch.save([losses, losses_mf], loss_file)\n",
    "torch.save([q_theta, q_theta_mf], q_file)\n",
    "torch.save([times, times_mf], time_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_str = 'data/dt_01_t_100000_minibatch_logit_theta_trunc_trans/SCON_C_{}.pt'\n",
    "loss_file = save_str.format('loss_mini1k')\n",
    "q_file = save_str.format('q_mini1k')\n",
    "time_file = save_str.format('time_mini1k')\n",
    "torch.save([losses, losses_mf, losses_trunc_mf], loss_file)\n",
    "torch.save([q_theta, q_theta_mf, q_theta_trunc_mf], q_file)\n",
    "torch.save([times, times_mf, times_trunc_mf], time_file)\n",
    "\n",
    "#loss_file = 'data/dt_1_t_100000_n_1_minibatch/theta_from_multi_x_loss_mini5k_scon_c_{}.pt'.format(seed)\n",
    "#q_file = 'data/dt_1_t_100000_n_1_minibatch/theta_from_multi_x_q_mini5k_scon_c_{}.pt'.format(seed)\n",
    "#time_file = 'data/dt_1_t_100000_n_1_minibatch/theta_from_multi_x_time_mini5k_scon_c_{}.pt'.format(seed)\n",
    "#torch.save([losses_mini5k, losses_mf_mini5k], loss_file)\n",
    "#torch.save([q_theta_mini5k, q_theta_mf_mini5k], q_file)\n",
    "#torch.save([times_mini5k, times_mf_mini5k], time_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_theta_full, q_theta_mf_full = torch.load('data/dt_1_t_100000_n_1_minibatch/theta_from_multi_x_q_full_scon_c_{}.pt'.format(seed))\n",
    "losses_full, losses_mf_full = torch.load('data/dt_1_t_100000_n_1_minibatch/theta_from_multi_x_loss_full_scon_c_{}.pt'.format(seed))\n",
    "times_full, times_mf_full = torch.load('data/dt_1_t_100000_n_1_minibatch/theta_from_multi_x_time_full_scon_c_{}.pt'.format(seed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_theta_mini5k, q_theta_mf_mini5k = torch.load('data/dt_1_t_100000_n_1_minibatch/theta_from_multi_x_q_mini5k_scon_c_{}.pt'.format(seed))\n",
    "losses_mini5k, losses_mf_mini5k = torch.load('data/dt_1_t_100000_n_1_minibatch/theta_from_multi_x_loss_mini5k_scon_c_{}.pt'.format(seed))\n",
    "times_mini5k, times_mf_mini5k = torch.load('data/dt_1_t_100000_n_1_minibatch/theta_from_multi_x_time_mini5k_scon_c_{}.pt'.format(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_theta_mini1k, q_theta_mf_mini1k = torch.load('data/dt_1_t_100000_n_1_minibatch/theta_from_multi_x_q_mini1k_scon_c_{}.pt'.format(seed))\n",
    "losses_mini1k, losses_mf_mini1k = torch.load('data/dt_1_t_100000_n_1_minibatch/theta_from_multi_x_loss_mini1k_scon_c_{}.pt'.format(seed))\n",
    "times_mini1k, times_mf_mini1k = torch.load('data/dt_1_t_100000_n_1_minibatch/theta_from_multi_x_time_mini1k_scon_c_{}.pt'.format(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_theta, q_theta_mf = torch.load('data/dt_1_t_1000000_n_1_minibatch/theta_from_x_q_scon_c_{}.pt'.format(seed))\n",
    "losses, losses_mf = torch.load('data/dt_1_t_1000000_n_1_minibatch/theta_from_x_loss_scon_c_{}.pt'.format(seed))\n",
    "times, times_mf = torch.load('data/dt_1_t_1000000_n_1_minibatch/theta_from_x_time_scon_c_{}.pt'.format(seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with unobserved $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_theta_y_file = '../training_pt_outputs/q_theta_iter_310000_t_1000_dt_1.0_batch_40_layers_5_lr_0.0005_sd_scale_0.333_SCON-C_logit_multi_2021_09_22_11_48_46.pt'\n",
    "q_theta_y_mf_file = '../training_pt_outputs/q_theta_iter_250000_t_1000_dt_1.0_batch_45_layers_5_lr_0.0005_sd_scale_0.333_SCON-C_no_CO2_logit_alt_2021_09_23_07_08_19.pt'\n",
    "device=torch.device('cpu')\n",
    "q_theta_y = extract_dist(torch.load(q_theta_y_file, map_location=device))\n",
    "q_theta_y_mf = extract_dist(torch.load(q_theta_y_mf_file, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_file = '../generated_data/SCON-C_CO2_logit_alt_sample_y_t_1000_dt_0-01_sd_scale_0-333_rsample.pt'\n",
    "labels = ['full-rank, observe $y$', 'full-rank, observe $x$', 'mean-field, observe $y$', 'mean-field observe $x$']\n",
    "plot_theta(p_theta, [q_theta_y, q_theta, q_theta_y_mf, q_theta_mf], theta_file, labels, q_theta_raw.keys,\n",
    "           colors=[cm.tab10(1), cm.tab10(1), cm.tab10(2), cm.tab10(2)], linestyles=['-', '--', '-', '--'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr([q_theta_y, q_theta], ['Observe $y$', 'Observe $x$'], q_theta_raw.keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OMGGG I just realized `SBM_SDE` would need to change as well to support theta-from-multiple-x’s inference. We need to compute the negative ELBO:\n",
    "$$\\mathcal{L} = - \\frac{1}{S} \\sum_s \\left[ \\log p(\\theta^{(s)}) + \\log p(x|\\theta^{(s)}) - \\log q(\\theta^{(s)}) \\right]$$\n",
    "where $S$ is the batch size.\n",
    "\n",
    "With $M$ sequences of $x_i, i = 1, ..., M$, the log likelihood term is now:\n",
    "$$\\log p(x|\\theta^{(s)}) = \\sum_i \\log p(x_i|\\theta^{(s)})$$\n",
    "\n",
    "So drift diffusion needs to take `C_PATH` of size `(S, M, N, D)` (currently `(S, N, D)`) and return a log likelihood tensor of size `(S, M)` (currently `(S, )`), where $N$ is the number of time steps and $D$ is the state dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(batch_size, 1, 1) \n",
    "diff1 = torch.arange(5).reshape((-1, 1, 1))\n",
    "diff2 = torch.arange(5, 10).reshape((-1, 1, 1))\n",
    "diff3 = torch.arange(10, 15).reshape((-1, 1, 1))\n",
    "diff_list = [diff1, diff2, diff3]\n",
    "diff_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = \n",
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_tensor = torch.diag_embed(torch.sqrt(LowerBound.apply(torch.cat(diff_list, 2), 1e-8))) # (batch_size, 1, state_dim, state_dim)\n",
    "diff_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_tensor[1] == torch.sqrt(torch.diag(torch.tensor([1, 6, 11])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(batch_size, N-1, state_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2, 3).reshape(2, 1, -1)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.diag_embed(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(12).reshape((2, 2, 3))\n",
    "b = torch.arange(4).reshape((2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_minibatches = 4\n",
    "\n",
    "minibatch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5000/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_size = n // 3\n",
    "batch_indices = torch.arange(num_minibatches) * minibatch_size\n",
    "batch_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = x[0]\n",
    "x0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch.randint(num_minibatches, (niter, ))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x_i[max(0, batch_indices[batch] - 1):batch_indices[batch + 1]] for x_i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0[batch_indices[batch0]-1:batch_indices[batch0+1]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.gather(x, 0, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_size = 1000\n",
    "t - minibatch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_indices = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_indices[torch.randint(len(start_indices), (niter, ))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.all(torch.bincount(torch.randint(len(start_indices), (niter, ))) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = torch.randint(100001, (niter, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.bincount(start_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_indices = torch.arange(0, t + 1 - minibatch_size, 100) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_indices[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.arange(0, t - minibatch_size, 100) + 1)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = {1, 2, 3}\n",
    "l = []\n",
    "l.extend(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float('123.45')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
