{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hack to import from a parent directory\n",
    "import sys\n",
    "path = '..'\n",
    "if path not in sys.path:\n",
    "    sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python-related imports\n",
    "import math, sys\n",
    "from typing import Dict, Tuple, Union\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "\n",
    "#Torch-related imports\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch import nn\n",
    "import torch.distributions as D\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#Module imports\n",
    "from SBM_SDE_classes_optim import *\n",
    "from obs_and_flow import *\n",
    "from training import *\n",
    "from plotting import *\n",
    "from mean_field import *\n",
    "from TruncatedNormal import *\n",
    "from LogitNormal import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import time\n",
    "from scipy.optimize import bisect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data\n",
    "\n",
    "### Draw $\\theta \\sim p(\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "temp_ref = 283\n",
    "temp_rise = 5 #High estimate of 5 celsius temperature rise by 2100.\n",
    "\n",
    "prior_scale_factor = 0.333\n",
    "\n",
    "#Parameter prior means\n",
    "u_M_mean = 0.0016\n",
    "a_SD_mean = 0.5\n",
    "a_DS_mean = 0.5\n",
    "a_M_mean = 0.5\n",
    "a_MSC_mean = 0.5\n",
    "k_S_ref_mean = 0.0005\n",
    "k_D_ref_mean = 0.0008\n",
    "k_M_ref_mean = 0.0007\n",
    "Ea_S_mean = 55\n",
    "Ea_D_mean = 48\n",
    "Ea_M_mean = 48\n",
    "c_SOC_mean = 0.5\n",
    "c_DOC_mean = 0.01\n",
    "c_MBC_mean = 0.01\n",
    "\n",
    "#SCON theta logit-normal distribution parameter details in order of mean, sdev, lower, and upper.\n",
    "u_M_details = torch.Tensor([u_M_mean, u_M_mean * prior_scale_factor, 0, 1])\n",
    "a_SD_details = torch.Tensor([a_SD_mean, a_SD_mean * prior_scale_factor, 0, 1])\n",
    "a_DS_details = torch.Tensor([a_DS_mean, a_DS_mean * prior_scale_factor, 0, 1])\n",
    "a_M_details = torch.Tensor([a_M_mean, a_M_mean * prior_scale_factor, 0, 1])\n",
    "a_MSC_details = torch.Tensor([a_MSC_mean, a_MSC_mean * prior_scale_factor, 0, 1])\n",
    "k_S_ref_details = torch.Tensor([k_S_ref_mean, k_S_ref_mean * prior_scale_factor, 0, 1])\n",
    "k_D_ref_details = torch.Tensor([k_D_ref_mean, k_D_ref_mean * prior_scale_factor, 0, 1])\n",
    "k_M_ref_details = torch.Tensor([k_M_ref_mean, k_M_ref_mean * prior_scale_factor, 0, 1])\n",
    "Ea_S_details = torch.Tensor([Ea_S_mean, Ea_S_mean * prior_scale_factor, 10, 100])\n",
    "Ea_D_details = torch.Tensor([Ea_D_mean, Ea_D_mean * prior_scale_factor, 10, 100])\n",
    "Ea_M_details = torch.Tensor([Ea_M_mean, Ea_M_mean * prior_scale_factor, 10, 100])\n",
    "\n",
    "#SCON-C diffusion matrix parameter distribution details\n",
    "c_SOC_details = torch.Tensor([c_SOC_mean, c_SOC_mean * prior_scale_factor, 0, 1])\n",
    "c_DOC_details = torch.Tensor([c_DOC_mean, c_DOC_mean * prior_scale_factor, 0, 1])\n",
    "c_MBC_details = torch.Tensor([c_MBC_mean, c_MBC_mean * prior_scale_factor, 0, 1])\n",
    "\n",
    "priors = {'u_M': u_M_details, 'a_SD': a_SD_details, 'a_DS': a_DS_details, 'a_M': a_M_details, 'a_MSC': a_MSC_details, 'k_S_ref': k_S_ref_details, 'k_D_ref': k_D_ref_details, 'k_M_ref': k_M_ref_details, 'Ea_S': Ea_S_details, 'Ea_D': Ea_D_details, 'Ea_M': Ea_M_details,\n",
    "          'c_SOC': c_SOC_details, 'c_DOC': c_DOC_details, 'c_MBC': c_MBC_details}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_scale(scale, loc, a, b, target_sd):\n",
    "    x = RescaledLogitNormal(loc, scale, a, b)\n",
    "    #print(scale, x.mean, x.stddev)\n",
    "    return x.stddev - target_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_theta(priors):\n",
    "    torch.manual_seed(0)\n",
    "    scale_lower = 1e-8 #Lower bound for scale search by bisect function.\n",
    "    scale_upper = 100 #Upper bound for scale search by bisect function. \n",
    "    \n",
    "    theta_hyperparams = {} # hyperparams\n",
    "    theta_samples = {} # theta samples\n",
    "    for k, v in priors.items():\n",
    "        sigmoid_loc, target_sd, a, b = v\n",
    "        loc = logit(sigmoid_loc, a, b)\n",
    "        scale = bisect(find_scale, scale_lower, scale_upper, (loc, a, b, target_sd))\n",
    "        dist = RescaledLogitNormal(loc, scale, a, b)\n",
    "        assert torch.abs(dist.stddev - target_sd) < 1e-6\n",
    "        \n",
    "        theta_hyperparams[k] = torch.tensor((loc, scale, a, b))\n",
    "        theta_samples[k] = dist.sample()\n",
    "        \n",
    "    return theta_hyperparams, theta_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_hyperparams, theta_samples = sample_theta(priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'u_M': tensor([-6.4362,  0.3104,  0.0000,  1.0000]), 'a_SD': tensor([0.0000, 0.7465, 0.0000, 1.0000]), 'a_DS': tensor([0.0000, 0.7465, 0.0000, 1.0000]), 'a_M': tensor([0.0000, 0.7465, 0.0000, 1.0000]), 'a_MSC': tensor([0.0000, 0.7465, 0.0000, 1.0000]), 'k_S_ref': tensor([-7.6004,  0.3100,  0.0000,  1.0000]), 'k_D_ref': tensor([-7.1301,  0.3101,  0.0000,  1.0000]), 'k_M_ref': tensor([-7.2637,  0.3101,  0.0000,  1.0000]), 'Ea_S': tensor([  0.0000,   0.9685,  10.0000, 100.0000]), 'Ea_D': tensor([ -0.3137,   0.8252,  10.0000, 100.0000]), 'Ea_M': tensor([ -0.3137,   0.8252,  10.0000, 100.0000]), 'c_SOC': tensor([0.0000, 0.7465, 0.0000, 1.0000]), 'c_DOC': tensor([-4.5951,  0.3137,  0.0000,  1.0000]), 'c_MBC': tensor([-4.5951,  0.3137,  0.0000,  1.0000])}\n"
     ]
    }
   ],
   "source": [
    "hyperparams_file = 'data/dt_1_t_100000_n_1_minibatch/theta_from_multi_x_hyperparams_scon_c_{}.pt'.format(0)\n",
    "theta_file = 'data/dt_1_t_100000_n_1_minibatch/theta_from_multi_x_theta_scon_c_{}.pt'.format(0)\n",
    "theta_hyperparams1 = torch.load(hyperparams_file)\n",
    "theta_samples1 = torch.load(theta_file)\n",
    "print(theta_hyperparams1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([True, True, True, True]),\n",
       " tensor([True, True, True, True]),\n",
       " tensor([True, True, True, True]),\n",
       " tensor([True, True, True, True]),\n",
       " tensor([True, True, True, True]),\n",
       " tensor([ True, False,  True,  True]),\n",
       " tensor([ True, False,  True,  True]),\n",
       " tensor([ True, False,  True,  True]),\n",
       " tensor([True, True, True, True]),\n",
       " tensor([True, True, True, True]),\n",
       " tensor([True, True, True, True]),\n",
       " tensor([True, True, True, True]),\n",
       " tensor([True, True, True, True]),\n",
       " tensor([True, True, True, True])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[theta_hyperparams[k] == theta_hyperparams1[k] for k in theta_hyperparams.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'data/dt_01_t_100000_minibatch_logit_theta_trunc_trans/SCON_C'\n",
    "target_hyperparams_file = '{}_target_hyperparams.pt'.format(save_dir)\n",
    "torch.save(priors, target_hyperparams_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw $x \\sim p(x|\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sequences = 1\n",
    "dt = 1.0\n",
    "t = 1000000\n",
    "x0_SCON = [65, 0.4, 2.5]\n",
    "x0_scale = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate data from SBM SDEs\n",
    "#x in order of SOC, DOC, MBC (and EEC for AWB family models)\n",
    "\n",
    "def alpha_SCON_multi(x, SCON_params_dict, I_S, I_D, current_temp, temp_ref, arrhenius_temp, linear_temp):\n",
    "    #Partition SOC, DOC, and MBC values.\n",
    "    state_dim = 3\n",
    "    SOC, DOC, MBC = torch.chunk(x, state_dim, 1)\n",
    "    \n",
    "    #Force temperature-dependent parameters.\n",
    "    k_S = arrhenius_temp(SCON_params_dict['k_S_ref'], current_temp, SCON_params_dict['Ea_S'], temp_ref)\n",
    "    k_D = arrhenius_temp(SCON_params_dict['k_D_ref'], current_temp, SCON_params_dict['Ea_D'], temp_ref)\n",
    "    k_M = arrhenius_temp(SCON_params_dict['k_M_ref'], current_temp, SCON_params_dict['Ea_M'], temp_ref)\n",
    "    \n",
    "    #Evolve drift.\n",
    "    drift_SOC = I_S + SCON_params_dict['a_DS'] * k_D * DOC + SCON_params_dict['a_M'] * SCON_params_dict['a_MSC'] * k_M * MBC - k_S * SOC\n",
    "    drift_DOC = I_D + SCON_params_dict['a_SD'] * k_S * SOC + SCON_params_dict['a_M'] * (1 - SCON_params_dict['a_MSC']) * k_M * MBC - (SCON_params_dict['u_M'] + k_D) * DOC\n",
    "    drift_MBC = SCON_params_dict['u_M'] * DOC - k_M * MBC\n",
    "    \n",
    "    return torch.cat([drift_SOC, drift_DOC, drift_MBC], 1)\n",
    "\n",
    "def beta_SCON_C_multi(x, SCON_C_params_dict):\n",
    "    b11 = SCON_C_params_dict['c_SOC']\n",
    "    b22 = SCON_C_params_dict['c_DOC']\n",
    "    b33 = SCON_C_params_dict['c_MBC']\n",
    "    b_matrix = torch.diag_embed(torch.stack([b11, b22, b33])) \n",
    "    return b_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_x(BATCH_SIZE, ALPHA, BETA, X0_LOC, X0_SCALE, T, DT, THETA_DICT, I_S_FUNC, I_D_FUNC, TEMP_FUNC, TEMP_REF, TEMP_RISE, lower_bound = 1e-4):\n",
    "    torch.manual_seed(seed)\n",
    "    if ALPHA == alpha_SCON_multi:\n",
    "        state_dim = 3\n",
    "    elif ALPHA == alpha_SAWB_multi:\n",
    "        state_dim = 4\n",
    "    elif ALPHA == alpha_SAWB_ECA_multi:\n",
    "        state_dim = 4\n",
    "        \n",
    "    N = int(T / DT) + 1\n",
    "    x = torch.zeros([BATCH_SIZE, N, state_dim])\n",
    "    \n",
    "    # Draw initial condition x0\n",
    "    X0_LOC = torch.as_tensor(X0_LOC)\n",
    "    p_x0 = D.multivariate_normal.MultivariateNormal(loc = X0_LOC,\n",
    "                                                    scale_tril = torch.diag(X0_LOC * X0_SCALE))\n",
    "    x0_samples = p_x0.sample((BATCH_SIZE, )) # (batch_size, state_dim)\n",
    "    #x0_samples[x0_samples < lower_bound] = lower_bound #Bound initial conditions above 0. \n",
    "    print('X0_samples = ', x0_samples)\n",
    "    x[:, 0, :] = x0_samples\n",
    "    \n",
    "    # Vectorize variable calculations where possible\n",
    "    hours = torch.tensor(np.linspace(0, T, N), dtype=torch.float) # 0\n",
    "    I_S_tensor = I_S_FUNC(hours)\n",
    "    I_D_tensor = I_D_FUNC(hours)\n",
    "    temps = TEMP_FUNC(hours, TEMP_REF, TEMP_RISE)\n",
    "    \n",
    "    #Take Euler-Maruyama step. \n",
    "    for i in range(1, N):\n",
    "        # Define x_i distribution\n",
    "        a = ALPHA(x[:, i - 1, :], THETA_DICT, I_S_tensor[i], I_D_tensor[i], temps[i], TEMP_REF, arrhenius_temp_dep, linear_temp_dep)\n",
    "        b = BETA(x[:, i - 1, :], THETA_DICT)\n",
    "        p_x_i = D.multivariate_normal.MultivariateNormal(loc = x[:, i - 1, :] + a * DT, covariance_matrix = b * DT)\n",
    "        \n",
    "        # Draw sample\n",
    "        x[:, i, :] = p_x_i.sample()        \n",
    "        #x[:, i, :][x[:, i, :] < lower_bound] = lower_bound #Bound all x above 0.\n",
    "    \n",
    "    return x, p_x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "x, p_x0 = generate_x(num_sequences, alpha_SCON_multi, beta_SCON_C_multi, x0_SCON, x0_scale, t, dt, theta_samples, i_s, i_d, temp_gen, temp_ref, temp_rise)\n",
    "print(x.shape, time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_file = 'data/dt_1_t_100000_n_1_minibatch/theta_from_multi_x_x_scon_c_{}.pt'.format(0)\n",
    "x = torch.load(x_file)\n",
    "x0_loc = torch.as_tensor(x0_SCON)\n",
    "p_x0 = D.multivariate_normal.MultivariateNormal(loc = x0_loc,\n",
    "                                                scale_tril = torch.diag(x0_loc * x0_scale))\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $x$ be the observed states (i.e. SOC, DOC, and MBC in the SCON-SS model) and $\\theta$ be the model parameters. The generative model is:\n",
    "- $\\theta \\sim \\text{LogitNormal}(\\mu_\\theta, \\sigma_\\theta, a, b)$\n",
    "- $x_1 \\sim p(x_1|\\theta)$\n",
    "- For $i=2, ..., T$: $x_i \\sim p(x_i|x_{i-1}, \\theta)$\n",
    "\n",
    "We use variational posterior $q_\\phi(\\theta)=\\text{MultivariateLogitNormal}(\\hat{\\mu}, \\hat{L}\\hat{L}^T, a, b)$ with variational parameter $\\phi=(\\hat{\\mu}, \\hat{L})$. For minibatching, we partition the sequence $x$ into $B$ consecutive subsequences $x_b$. We sample a minibatch by drawing $b$ uniformly from $1, ..., B$. Let $u_b$ and $v_b$ denote the endpoints of the $b$th subsequence, $x_b = x_{u_b:v_b}$. The minibatch loss is:\n",
    "$$\n",
    "\\mathcal{L} = E_{q_\\phi(\\theta)}\\left[ \\log p(\\theta) - \\log q(\\theta) + \\log p(x_b|\\theta) \\right] \\\\\n",
    "$$\n",
    "\n",
    "where the log likelihood term is given by:\n",
    "$$\n",
    "\\log p(x_b|\\theta) = \\sum_{i=u_b}^{v_b} \\log p(x_i|x_{i-1}, \\theta)\n",
    "$$\n",
    "if $u_b > 1$, otherwise: $\\log p(x_b|\\theta) = \\log p(x_1|\\theta) + \\sum_{i=u_b+1}^{v_b} \\log p(x_i|x_{i-1}, \\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reparameterized gradient wrt variational parameter $\\phi$:\n",
    "$$\n",
    "\\nabla_\\phi \\mathcal{L} \\approx -\\frac{1}{S} \\sum_s \\nabla_\\phi \\left[\n",
    "    \\log p(g_\\phi(\\epsilon^{(s)})) + \\log p(x_b|g_\\phi(\\epsilon^{(s)})) - \\log q(g_\\phi(\\epsilon^{(s)}))\n",
    "    \\right]\n",
    "$$\n",
    "\n",
    "where $\\epsilon^{(s)} \\sim \\text{MultivariateNormal}(0, I)$ and $g_\\phi(\\epsilon)=(b-a) \\odot \\sigma(\\hat{\\mu}+\\hat{L}\\epsilon)+a$, with $\\odot$ to denote elementwise multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudocode:\n",
    "- for each iteration $n=1, ..., N$:\n",
    "  - Sample minibatch: $b \\sim \\text{Uniform}(1, ..., B)$\n",
    "  - Sample $\\theta$: $\\theta^{(s)} \\sim q(\\theta)$ for $s=1, ..., S$\n",
    "  - `loss` $= -\\frac{1}{S} \\sum_s \\left[\\log p(\\theta^{(s)}) + \\log p(x_b|\\theta^{(s)}) - \\log q(\\theta^{(s)})\\right]$ \n",
    "  - `loss.backward()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation note:** The covariance matrix $\\Sigma$ of the multivariate normal in PyTorch is parameterized in terms of a lower-triangular matrix $L$ with positive-valued diagonal entries, such that $\\Sigma = LL^T$. In practice, we optimize the unconstrained transformation of $L$. The following code shows how the mapping is done back and forth:\n",
    "```\n",
    "def to_constrained(self, x):\n",
    "    # Takes unconstrained square matrix x and returns L, s.t. Sigma = LL^T\n",
    "    return x.tril(-1) + x.diagonal(dim1=-2, dim2=-1).exp().diag_embed()\n",
    "\n",
    "def to_unconstrained(self, y):\n",
    "    return y.tril(-1) + y.diagonal(dim1=-2, dim2=-1).log().diag_embed()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_flow = 1.0 # [1.0, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "torch.set_printoptions(precision = 8)\n",
    "\n",
    "#Neural SDE parameters\n",
    "n = int(t / dt_flow) + 1\n",
    "t_span = np.linspace(0, t, n)\n",
    "t_span_tensor = torch.reshape(torch.Tensor(t_span), [1, n, 1]).to(active_device) #T_span needs to be converted to tensor object. Additionally, facilitates conversion of I_S and I_D to tensor objects.\n",
    "\n",
    "#Specify desired SBM SDE model type and details.\n",
    "SBM_SDE_class = 'SCON'\n",
    "diffusion_type = 'C'\n",
    "theta_dist = 'RescaledLogitNormal' #String needs to be exact name of the distribution class. Other option is 'RescaledLogitNormal'.\n",
    "theta_post_dist = 'MultivariateLogitNormal'\n",
    "\n",
    "#Generate exogenous input vectors.\n",
    "#Obtain temperature forcing function.\n",
    "temp_tensor = temp_gen(t_span_tensor, temp_ref, temp_rise).to(active_device)\n",
    "\n",
    "#Obtain SOC and DOC pool litter input vectors for use in flow SDE functions.\n",
    "i_s_tensor = i_s(t_span_tensor).to(active_device) #Exogenous SOC input function\n",
    "i_d_tensor = i_d(t_span_tensor).to(active_device) #Exogenous DOC input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_log_lik(C_PATH, PARAMS_DICT, DT, SBM_SDE_CLASS, INIT_PRIOR, start_idx, end_idx, calc_full=False):      \n",
    "    # Compute drift and diffusion\n",
    "    if calc_full:\n",
    "        drift, diffusion_sqrt = SBM_SDE_CLASS.drift_diffusion(C_PATH, PARAMS_DICT)\n",
    "        p_x = D.multivariate_normal.MultivariateNormal(loc = C_PATH[:, :-1, :] + drift * DT,\n",
    "                                                       scale_tril = diffusion_sqrt * math.sqrt(DT)) \n",
    "    \n",
    "        # Compute log p(x|theta) = log p(x|x0, theta) + log p(x0|theta)\n",
    "        log_p_x = p_x.log_prob(C_PATH[:, 1:, :]) # log p(x_i|x_{i-1}, theta), (batch_size, N - 1)\n",
    "        log_p_x0 = INIT_PRIOR.log_prob(C_PATH[:, 0, :]) # log p(x0|theta), (batch_size, )\n",
    "        ll_minibatch = log_p_x[:, start_idx:end_idx-1].sum(-1)\n",
    "        if start_idx == 0:\n",
    "            ll_minibatch += log_p_x0\n",
    "        ll_full = log_p_x.sum(-1) + log_p_x0 # log p(x|theta), (batch_size, )\n",
    "    \n",
    "    else:\n",
    "        drift, diffusion_sqrt = SBM_SDE_CLASS.drift_diffusion(C_PATH, PARAMS_DICT, start_idx, end_idx) \n",
    "        p_x = D.multivariate_normal.MultivariateNormal(loc = C_PATH[:, start_idx:end_idx-1, :] + drift * DT,\n",
    "                                                       scale_tril = diffusion_sqrt * math.sqrt(DT))\n",
    "    \n",
    "        # Compute log p(x|theta) = log p(x|x0, theta) + log p(x0|theta)\n",
    "        ll_minibatch = p_x.log_prob(C_PATH[:, start_idx+1:end_idx, :]).sum(-1) # log p(x|x0, theta)\n",
    "        if start_idx == 0:\n",
    "            ll_minibatch += INIT_PRIOR.log_prob(C_PATH[:, 0, :]) # log p(x0|theta)\n",
    "        ll_full = None\n",
    "    \n",
    "    return ll_minibatch, ll_full\n",
    "\n",
    "def train(DEVICE, LR, NITER, BATCH_SIZE, MINIBATCH_SIZE, X_ALL, T, DT, N,\n",
    "          T_SPAN_TENSOR, I_S_TENSOR, I_D_TENSOR, TEMP_TENSOR, TEMP_REF,\n",
    "          SBM_SDE_CLASS, DIFFUSION_TYPE, X0_PRIOR, PRIOR_DIST_DETAILS_DICT, \n",
    "          THETA_DIST = None, THETA_POST_DIST = None, THETA_POST_INIT = None,\n",
    "          LR_DECAY = 0.8, DECAY_STEP_SIZE = 50000, PRINT_EVERY = 100, MINIBATCH_INDICES=None,\n",
    "          CALC_LOSS_EVERY=100):\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Instantiate SBM_SDE object based on specified model and diffusion type.\n",
    "    SBM_SDE_class_dict = {\n",
    "            'SCON': SCON_optim,\n",
    "            'SAWB': SAWB_optim,\n",
    "            'SAWB-ECA': SAWB_ECA_optim\n",
    "            }\n",
    "    if SBM_SDE_CLASS not in SBM_SDE_class_dict:\n",
    "        raise NotImplementedError('Other SBM SDEs aside from SCON, SAWB, and SAWB-ECA have not been implemented yet.')\n",
    "    SBM_SDE_class = SBM_SDE_class_dict[SBM_SDE_CLASS]\n",
    "    SBM_SDE = SBM_SDE_class(T_SPAN_TENSOR, I_S_TENSOR, I_D_TENSOR, TEMP_TENSOR, TEMP_REF, DIFFUSION_TYPE)\n",
    "\n",
    "    # Load x, exclude CO2 and extra time steps \n",
    "    # (since data generation process uses smaller dt than dt_flow)\n",
    "    #x_all = torch.load(X_FILE)\n",
    "    step = (X_ALL.shape[1] - 1) // (N - 1)\n",
    "    x = X_ALL[:, ::step, :SBM_SDE.state_dim]\n",
    "    #x = x.T.expand(BATCH_SIZE, -1, -1)\n",
    "    assert x.shape == (1, N, SBM_SDE.state_dim)\n",
    "\n",
    "    # Convert prior details dictionary values to tensors.\n",
    "    param_names = list(PRIOR_DIST_DETAILS_DICT.keys())\n",
    "    prior_list = list(zip(*(PRIOR_DIST_DETAILS_DICT[k] for k in param_names))) #Unzip prior distribution details from dictionary values into individual lists.\n",
    "    prior_means_tensor, prior_sds_tensor, prior_lowers_tensor, prior_uppers_tensor = torch.tensor(prior_list).to(DEVICE) #Ensure conversion of lists into tensors.\n",
    "\n",
    "    # Retrieve desired distribution class based on string.\n",
    "    dist_class_dict = {\n",
    "            'TruncatedNormal': TruncatedNormal,\n",
    "            'RescaledLogitNormal': RescaledLogitNormal,\n",
    "            'MultivariateLogitNormal': MultivariateLogitNormal\n",
    "            }\n",
    "    THETA_PRIOR_CLASS = dist_class_dict[THETA_DIST]\n",
    "    THETA_POST_CLASS = dist_class_dict[THETA_POST_DIST] if THETA_POST_DIST else dist_class_dict[THETA_DIST]\n",
    "    \n",
    "    # Define prior\n",
    "    p_theta = THETA_PRIOR_CLASS(loc = prior_means_tensor, scale = prior_sds_tensor, a = prior_lowers_tensor, b = prior_uppers_tensor)\n",
    "\n",
    "    # Initialize posterior q(theta) using its prior p(theta)\n",
    "    learn_cov = (THETA_POST_DIST == 'MultivariateLogitNormal')\n",
    "    if THETA_POST_INIT is None:\n",
    "        THETA_POST_INIT = PRIOR_DIST_DETAILS_DICT\n",
    "    q_theta = MeanField(DEVICE, param_names, THETA_POST_INIT, THETA_POST_CLASS, learn_cov)\n",
    "\n",
    "    #Record loss throughout training (needs moving average bc of minibatching)\n",
    "    #best_loss = 1e15\n",
    "    losses = []\n",
    "    losses_full = []\n",
    "    times = []\n",
    "\n",
    "    #Initiate optimizers.\n",
    "    optimizer = optim.Adamax(list(q_theta.parameters()), lr = LR)\n",
    "    \n",
    "    # Sample minibatch endpoints\n",
    "    if MINIBATCH_SIZE < N:\n",
    "        if MINIBATCH_INDICES is None:\n",
    "            step = (N - MINIBATCH_SIZE) // NITER\n",
    "            MINIBATCH_INDICES = torch.arange(0, (N - MINIBATCH_SIZE), step)\n",
    "        rand = torch.randint(len(MINIBATCH_INDICES), (NITER, ))\n",
    "        print(torch.min(torch.bincount(rand)))\n",
    "        batch_indices = MINIBATCH_INDICES[rand]\n",
    "    \n",
    "    #Training loop\n",
    "    t0 = time.time()\n",
    "    with tqdm(total = NITER, desc = f'Learning SDE and hidden parameters.', position = -1) as tq:\n",
    "        for it in range(1, NITER + 1):\n",
    "            optimizer.zero_grad()    \n",
    "            \n",
    "            # Sample theta ~ q(theta) and compute log q(theta)\n",
    "            theta_dict, theta, log_q_theta, _ = q_theta(BATCH_SIZE)\n",
    "            \n",
    "            # Compute log p(theta)\n",
    "            log_p_theta = p_theta.log_prob(theta).sum(-1)\n",
    "\n",
    "            # Compute log p(x_{u-1:v}|theta) (unless u = 0, then x_{u:v})\n",
    "            if MINIBATCH_SIZE < N:\n",
    "                start_idx = max(0, batch_indices[it-1] - 1)              # u-1 if u > 0, else 0\n",
    "                end_idx = min(N, batch_indices[it-1] + MINIBATCH_SIZE)   # v\n",
    "            else:\n",
    "                start_idx, end_idx = 0, N\n",
    "            #print(batch_id, start_idx, end_idx)\n",
    "            calc_loss = (it % CALC_LOSS_EVERY == 0) or (it == 1)\n",
    "            ll, ll_full = calc_log_lik(x, theta_dict, DT, SBM_SDE, X0_PRIOR,\n",
    "                                       start_idx, end_idx, calc_loss)\n",
    "\n",
    "            # Compute negative ELBO: -(log p(theta) + log p(x|theta) - log q(theta))\n",
    "            loss = -log_p_theta.mean() - N/MINIBATCH_SIZE * ll.mean() + log_q_theta.mean()\n",
    "            losses.append(loss.item())\n",
    "            if calc_loss:\n",
    "                loss_full = -log_p_theta.mean() - ll_full.mean() + log_q_theta.mean()\n",
    "                losses_full.append(loss_full.item())\n",
    "\n",
    "            # Take a gradient step\n",
    "            loss.backward()\n",
    "            #torch.nn.utils.clip_grad_norm_(ELBO_params, 5.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Record time\n",
    "            times.append(time.time() - t0)\n",
    "            \n",
    "            if it % PRINT_EVERY == 0:\n",
    "                if calc_loss:\n",
    "                    print('Iteration {} loss: {}'.format(it, loss_full))\n",
    "                else:\n",
    "                    k = 1000\n",
    "                    ma_loss = losses[it-k-1:it-1].mean()\n",
    "                    print('Iteration {} moving average loss: {}'.format(it, ma_loss))\n",
    "        \n",
    "            if it % DECAY_STEP_SIZE == 0:\n",
    "                optimizer.param_groups[0]['lr'] *= LR_DECAY\n",
    "\n",
    "            tq.update()\n",
    "    \n",
    "    return q_theta, p_theta, losses, losses_full, times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training parameters\n",
    "niter = 200000 # niter = epochs * (n - 1) / minibatch_size\n",
    "train_lr = 0.01 #ELBO learning rate\n",
    "batch_size = 40 #3 - number needed to fit UCI HPC3 RAM requirements with 16 GB RAM at t = 5000.\n",
    "#minibatch_size = 1000\n",
    "#minibatch_indices = torch.arange(0, t + 1 - minibatch_size, 100) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "size5k = 5000\n",
    "indices5k = torch.arange(0, t + 1 - size5k, size5k) + 1\n",
    "#indices5k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "size1k = 1000\n",
    "indices1k = torch.arange(0, t + 1 - size1k, size1k) + 1\n",
    "#indices1k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call training loop function for SCON-C.\n",
    "t0 = time.time()\n",
    "q_theta_raw_mini5k, p_theta_mini5k, losses_noisy_mini5k, losses_mini5k, times_mini5k = train(\n",
    "        active_device, train_lr, niter, batch_size, size5k, x, t, dt_flow, n, \n",
    "        t_span_tensor, i_s_tensor, i_d_tensor, temp_tensor, temp_ref,\n",
    "        SBM_SDE_class, diffusion_type, p_x0, theta_hyperparams,\n",
    "        THETA_DIST=theta_dist, THETA_POST_DIST=theta_post_dist,\n",
    "        LR_DECAY = 1.0, PRINT_EVERY = max(1, niter // 10), MINIBATCH_INDICES=indices5k)\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_theta_raw, p_theta, losses_noisy, losses, times = q_theta_raw_mini5k, p_theta_mini5k, losses_noisy_mini5k, losses_mini5k, times_mini5k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call training loop function for SCON-C.\n",
    "t0 = time.time()\n",
    "q_theta_raw_mini1k, p_theta_mini1k, losses_noisy_mini1k, losses_mini1k, times_mini1k = train(\n",
    "        active_device, train_lr, niter, batch_size, size1k, x, t, dt_flow, n, \n",
    "        t_span_tensor, i_s_tensor, i_d_tensor, temp_tensor, temp_ref,\n",
    "        SBM_SDE_class, diffusion_type, p_x0, theta_hyperparams,\n",
    "        THETA_DIST=theta_dist, THETA_POST_DIST=theta_post_dist,\n",
    "        LR_DECAY = 1.0, PRINT_EVERY = max(1, niter // 10), MINIBATCH_INDICES=indices1k)\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call training loop function for SCON-C.\n",
    "t0 = time.time()\n",
    "q_theta_raw_mf_mini5k, p_theta_mf_mini5k, losses_mf_noisy_mini5k, losses_mf_mini5k, times_mf_mini5k = train(\n",
    "        active_device, train_lr, niter, batch_size, size5k, x, t, dt_flow, n, \n",
    "        t_span_tensor, i_s_tensor, i_d_tensor, temp_tensor, temp_ref,\n",
    "        SBM_SDE_class, diffusion_type, p_x0, theta_hyperparams,\n",
    "        THETA_DIST=theta_dist, THETA_POST_DIST=theta_dist,\n",
    "        LR_DECAY = 1.0, PRINT_EVERY = max(1, niter // 10), MINIBATCH_INDICES=indices5k)\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_theta_raw_mf, p_theta_mf, losses_mf_noisy, losses_mf, times_mf = q_theta_raw_mf_mini5k, p_theta_mf_mini5k, losses_mf_noisy_mini5k, losses_mf_mini5k, times_mf_mini5k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call training loop function for SCON-C.\n",
    "t0 = time.time()\n",
    "q_theta_raw_mf_mini1k, p_theta_mf_mini1k, losses_mf_noisy_mini1k, losses_mf_mini1k, times_mf_mini1k = train(\n",
    "        active_device, train_lr, niter, batch_size, size1k, x, t, dt_flow, n, \n",
    "        t_span_tensor, i_s_tensor, i_d_tensor, temp_tensor, temp_ref,\n",
    "        SBM_SDE_class, diffusion_type, p_x0, theta_hyperparams,\n",
    "        THETA_DIST=theta_dist, THETA_POST_DIST=theta_dist,\n",
    "        LR_DECAY = 1.0, PRINT_EVERY = max(1, niter // 10), MINIBATCH_INDICES=indices1k)\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(x, dim=-1, n=1000):\n",
    "    cumsum = torch.cumsum(torch.as_tensor(x), dim)\n",
    "    avg0 = cumsum[:n-1]/torch.arange(1, n)\n",
    "    avg = (cumsum[n-1:] - cumsum[:-(n-1)])/n\n",
    "    return torch.cat((avg0, avg)).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_ma = moving_average(losses)\n",
    "losses_mf_ma = moving_average(losses_mf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(loss_hist_list, labels, xscale='linear', ymin=None, ymax=None,\n",
    "              colors=None, linestyles=None, xvals=None, xlabel='iteration'):\n",
    "    plt.rcParams.update({'font.size': 12, 'lines.linewidth': 2, 'figure.figsize': (8, 6)})\n",
    "    if colors is None:\n",
    "        colors = [cm.tab10(i+1) for i in range(len(labels))]\n",
    "    if linestyles is None:\n",
    "        linestyles = ['-'] * len(labels)\n",
    "    if xvals is None:\n",
    "        xvals = [None] * len(labels)\n",
    "    \n",
    "    for loss_hist, x, label, c, s in zip(loss_hist_list, xvals, labels, colors, linestyles):\n",
    "        if x is None:\n",
    "            plt.plot(loss_hist, label=label, color=c, linestyle=s)\n",
    "        else:\n",
    "            plt.plot(x, loss_hist, label=label, color=c, linestyle=s)\n",
    "    \n",
    "    plt.xlabel(xlabel)\n",
    "    #plt.title('Loss v iteration')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.xscale(xscale)\n",
    "    plt.ylim((ymin, ymax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['full-rank', 'mean-field']\n",
    "plot_loss([losses, losses_mf], labels, ymax=-615000, ymin=-625000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch = iter * minibatch_size / (n - 1)\n",
    "epochs = [torch.arange(0, niter + 1, 100) * size5k / (n - 1)] * 2\n",
    "plot_loss([losses, losses_mf], labels, ymax=-615000, ymin=-625000, xvals=epochs, xlabel='epoch', xscale='symlog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_indices = torch.cat((torch.tensor([0]), torch.arange(99, niter + 1, 100)))\n",
    "time_vals = [torch.tensor(times)[time_indices], torch.tensor(times_mf)[time_indices]]\n",
    "plot_loss([losses, losses_mf], labels, ymax=-615000, ymin=-625000, xvals=time_vals, xlabel='time', xscale='symlog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['full-rank (full sequence)', 'mean-field (full sequence)',\n",
    "          'full-rank (minibatch size = 5,000)', 'mean-field (minibatch size = 5,000)',\n",
    "          'full-rank (minibatch size = 1,000)', 'mean-field (minibatch size = 1,000)']\n",
    "colors =  [cm.tab10(i) for i in [1, 2, 1, 2, 1, 2]]\n",
    "linestyles = ['-', '-', '--', '--', ':', ':']\n",
    "\n",
    "epochs = [None] * 2 + \\\n",
    "         [torch.arange(0, niter + 1, 100) * size5k / (n - 1)] * 2 + \\\n",
    "         [torch.arange(0, niter + 1, 100) * size1k / (n - 1)] * 2\n",
    "plot_loss([losses_full, losses_mf_full, losses_mini5k, losses_mf_mini5k, losses_mini1k, losses_mf_mini1k],\n",
    "          labels, ymax=-61000, ymin=-62200, \n",
    "          colors=colors, linestyles=linestyles, xvals=epochs, xscale='symlog', xlabel='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_indices = torch.cat((torch.tensor([0]), torch.arange(99, niter + 1, 100) ))\n",
    "time_vals = [times_full, times_mf_full] + [torch.tensor(times)[time_indices] for times in [times_mini5k, times_mf_mini5k, times_mini1k, times_mf_mini1k]]\n",
    "\n",
    "plot_loss([losses_full, losses_mf_full, losses_mini5k, losses_mf_mini5k, losses_mini1k, losses_mf_mini1k],\n",
    "          labels, ymax=-61000, ymin=-62200, xlabel='time', xscale='symlog',\n",
    "          colors=colors, linestyles=linestyles, xvals=time_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['full-rank (minibatch overlap)', 'mean-field (minibatch overlap)',\n",
    "          'full-rank (minibatch no overlap)', 'mean-field (minibatch no overlap)']\n",
    "colors =  [cm.tab10(i) for i in [1, 2, 1, 2]]\n",
    "linestyles = ['-', '-', '--', '--']\n",
    "\n",
    "xvals = [torch.arange(0, niter + 1, 100)] * 4\n",
    "plot_loss([losses_mini5k_over, losses_mf_mini5k_over, losses_mini5k, losses_mf_mini5k],\n",
    "          labels, ymax=-61000, ymin=-62200,\n",
    "          colors=colors, linestyles=linestyles, xvals=xvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the distribution from a MeanField object\n",
    "def extract_dist(q):\n",
    "    a, b = q.lowers, q.uppers\n",
    "    loc = q.means\n",
    "    if not q.learn_cov:\n",
    "        scale = torch.max(q.sds, torch.ones_like(q.sds) * 1e-8)\n",
    "        #scale = D.transform_to(q.dist.arg_constraints['scale'])(q.sds)\n",
    "        return q.dist(loc, scale=scale, a=a, b=b)\n",
    "    else:\n",
    "        scale = D.transform_to(q.dist.arg_constraints['scale_tril'])(q.sds)\n",
    "        return q.dist(loc, scale_tril=scale, a=a, b=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_theta_mini5k = extract_dist(q_theta_raw_mini5k)\n",
    "q_theta_mf_mini5k = extract_dist(q_theta_raw_mf_mini5k)\n",
    "q_theta_mini1k = extract_dist(q_theta_raw_mini1k)\n",
    "q_theta_mf_mini1k = extract_dist(q_theta_raw_mf_mini1k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_theta(p_theta, q_theta_list, theta, labels, param_names, num_pts=1000, eps=1e-5, ncols=4,\n",
    "               colors=None, linestyles=None, device=active_device):\n",
    "    plt.rcParams.update({'font.size': 16, 'lines.linewidth': 2})\n",
    "    \n",
    "    # Load posterior and define plot boundaries\n",
    "    a, b = p_theta.a, p_theta.b\n",
    "    x0 = p_theta.mean - 4*p_theta.stddev\n",
    "    x1 = p_theta.mean + 4*p_theta.stddev\n",
    "    q_marginals = []\n",
    "    for q_theta in q_theta_list:\n",
    "        if isinstance(q_theta, RescaledLogitNormal):\n",
    "            q_marginal = q_theta\n",
    "        else:\n",
    "            scale_post = torch.diag(q_theta.covariance_matrix).sqrt()\n",
    "            q_marginal = RescaledLogitNormal(q_theta.loc, scale_post, a=a, b=b)\n",
    "        q_marginals.append(q_marginal)\n",
    "        x0 = torch.fmin(x0, q_marginal.mean - 4*q_marginal.stddev)\n",
    "        x1 = torch.fmax(x1, q_marginal.mean + 4*q_marginal.stddev)\n",
    "        #print(x0, x1)\n",
    "    x0 = torch.fmax(x0, a).detach()\n",
    "    x1 = torch.fmin(x1, b).detach()\n",
    "    x = torch.from_numpy(np.linspace(x0, x1, num_pts))\n",
    "    \n",
    "    # Load true theta\n",
    "    #theta = torch.load(theta_file, map_location=device)\n",
    "    \n",
    "    # Compute pdfs\n",
    "    #print(x[0, :], x[-1, :])\n",
    "    prior_pdf = torch.exp(p_theta.log_prob(x)).detach()\n",
    "    post_pdfs = []\n",
    "    for q_theta in q_marginals:\n",
    "        post_pdf = torch.exp(q_theta.log_prob(x)).detach()\n",
    "        post_pdfs.append(post_pdf)\n",
    "    \n",
    "    # Plot prior v posterior v true theta\n",
    "    num_params = len(param_names)\n",
    "    nrows = int(num_params / ncols) + 1\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 4, nrows * 4))\n",
    "    axes = np.atleast_2d(axes)\n",
    "    k = 0\n",
    "    if colors is None: colors = [cm.tab10(i+1) for i in range(len(post_pdfs))]\n",
    "    if linestyles is None: linestyles = ['-'] * len(post_pdfs)\n",
    "    for i, row in enumerate(axes):\n",
    "        for j, ax in enumerate(row):\n",
    "            if k < num_params:\n",
    "                key = param_names[k]\n",
    "                ax.plot(x[:, k], prior_pdf[:, k], label='Prior', color='tab:blue')\n",
    "                for post_pdf, post_dist, c, l in zip(post_pdfs, labels, colors, linestyles):\n",
    "                    label = 'Posterior {}'.format(post_dist)\n",
    "                    ax.plot(x[:, k], post_pdf[:, k], label=label, color=c, linestyle=l)\n",
    "                ax.axvline(theta[key], color='gray', label='True $\\\\theta$')\n",
    "                ax.set_xlabel(key)\n",
    "                if j == 0: ax.set_ylabel('density')\n",
    "            elif k == num_params:\n",
    "                handles, labels = axes[0, 0].get_legend_handles_labels()\n",
    "                ax.legend(handles, labels, loc='center')\n",
    "                ax.axis('off')\n",
    "            else:\n",
    "                fig.delaxes(axes[i, j])\n",
    "            k += 1  \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Marginal distributions')\n",
    "    plt.subplots_adjust(top=0.95)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['full-rank (full)', 'mean-field (full)',\n",
    "          'full-rank (mini = 5,000)', 'mean-field (mini = 5,000)',\n",
    "          'full-rank (mini = 1,000)', 'mean-field (mini = 1,000)']\n",
    "plot_theta(p_theta_mini5k, [q_theta_full, q_theta_mf_full, q_theta_mini5k, q_theta_mf_mini5k, q_theta_mini1k, q_theta_mf_mini1k],\n",
    "           theta_samples, labels, q_theta_raw_mini5k.keys, colors=colors, linestyles=linestyles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_theta(p_theta, [q_theta, q_theta_mf],\n",
    "           theta_samples, labels, q_theta_raw.keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr(q_theta_list, labels, param_names, num_samples=100000):\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "    \n",
    "    # Calculate empirical correlation\n",
    "    corr_list = []\n",
    "    for q_theta in q_theta_list:\n",
    "        assert isinstance(q_theta, MultivariateLogitNormal)\n",
    "        samples = q_theta.sample((num_samples, )) # (N, D)\n",
    "        corr_mc = np.corrcoef(samples.T)\n",
    "        corr_list.append(corr_mc)\n",
    "    \n",
    "    # Plot\n",
    "    num_cols = len(q_theta_list)\n",
    "    fig, axes = plt.subplots(1, num_cols, figsize=(8*num_cols, 8))\n",
    "    axes = np.atleast_1d(axes)\n",
    "    D = len(q_theta_list[0].loc)\n",
    "    \n",
    "    for i, ax in enumerate(axes):\n",
    "        plot = ax.imshow(corr_list[i], cmap='coolwarm', vmin=-1, vmax=1)\n",
    "        ax.set_xticks(range(D))\n",
    "        ax.set_xticklabels(param_names, rotation='vertical')\n",
    "        ax.set_yticks(range(D))\n",
    "        ax.set_yticklabels(param_names)\n",
    "        ax.set_title(labels[i])\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.colorbar(plot, ax=axes, shrink=0.8)\n",
    "    plt.suptitle('Correlation between parameters')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_corr([q_theta_full, q_theta_mini5k, q_theta_mini1k],\n",
    "          ['full', 'minibatch size = 5,000', 'minibatch size = 1,000'], priors.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr([q_theta],\n",
    "          ['minibatch size = 5,000'], q_theta_raw.keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_x(x, t_span, n):\n",
    "    num_sequences, time_steps, state_dim = x.shape\n",
    "    fig, axes = plt.subplots(state_dim, figsize=(15, 15))\n",
    "    step = (x.shape[1] - 1) // (n - 1)\n",
    "    x_plot = x[:, ::step, :state_dim]\n",
    "    #print(x.shape, x_plot.shape)\n",
    "    \n",
    "    labels = ['SOC', 'DOC', 'MBC', 'EEC']\n",
    "    for i, ax in enumerate(axes):\n",
    "        for j in range(num_sequences):\n",
    "            ax.plot(t_span, x_plot[j, :, i])\n",
    "        ax.set_ylabel(labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_x(x, t_span, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams_file = 'data/dt_1_t_1000000_n_1_minibatch/theta_from_x_hyperparams_scon_c_{}.pt'.format(seed)\n",
    "theta_file = 'data/dt_1_t_1000000_n_1_minibatch/theta_from_x_theta_scon_c_{}.pt'.format(seed)\n",
    "x_file = 'data/dt_1_t_1000000_n_1_minibatch/theta_from_x_x_scon_c_{}.pt'.format(seed)\n",
    "loss_file = 'data/dt_1_t_1000000_n_1_minibatch/theta_from_x_loss_scon_c_{}.pt'.format(seed)\n",
    "q_file = 'data/dt_1_t_1000000_n_1_minibatch/theta_from_x_q_scon_c_{}.pt'.format(seed)\n",
    "time_file = 'data/dt_1_t_1000000_n_1_minibatch/theta_from_x_time_scon_c_{}.pt'.format(seed)\n",
    "torch.save(theta_hyperparams, hyperparams_file)\n",
    "torch.save(theta_samples, theta_file)\n",
    "torch.save(x, x_file)\n",
    "torch.save([losses, losses_mf], loss_file)\n",
    "torch.save([q_theta, q_theta_mf], q_file)\n",
    "torch.save([times, times_mf], time_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_file = 'data/dt_1_t_100000_n_1_minibatch/theta_from_multi_x_loss_mini1k_scon_c_{}.pt'.format(seed)\n",
    "q_file = 'data/dt_1_t_100000_n_1_minibatch/theta_from_multi_x_q_mini1k_scon_c_{}.pt'.format(seed)\n",
    "time_file = 'data/dt_1_t_100000_n_1_minibatch/theta_from_multi_x_time_mini1k_scon_c_{}.pt'.format(seed)\n",
    "torch.save([losses_mini1k, losses_mf_mini1k], loss_file)\n",
    "torch.save([q_theta_mini1k, q_theta_mf_mini1k], q_file)\n",
    "torch.save([times_mini1k, times_mf_mini1k], time_file)\n",
    "\n",
    "loss_file = 'data/dt_1_t_100000_n_1_minibatch/theta_from_multi_x_loss_mini5k_scon_c_{}.pt'.format(seed)\n",
    "q_file = 'data/dt_1_t_100000_n_1_minibatch/theta_from_multi_x_q_mini5k_scon_c_{}.pt'.format(seed)\n",
    "time_file = 'data/dt_1_t_100000_n_1_minibatch/theta_from_multi_x_time_mini5k_scon_c_{}.pt'.format(seed)\n",
    "torch.save([losses_mini5k, losses_mf_mini5k], loss_file)\n",
    "torch.save([q_theta_mini5k, q_theta_mf_mini5k], q_file)\n",
    "torch.save([times_mini5k, times_mf_mini5k], time_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_theta_full, q_theta_mf_full = torch.load('data/dt_1_t_100000_n_1_minibatch/theta_from_multi_x_q_full_scon_c_{}.pt'.format(seed))\n",
    "losses_full, losses_mf_full = torch.load('data/dt_1_t_100000_n_1_minibatch/theta_from_multi_x_loss_full_scon_c_{}.pt'.format(seed))\n",
    "times_full, times_mf_full = torch.load('data/dt_1_t_100000_n_1_minibatch/theta_from_multi_x_time_full_scon_c_{}.pt'.format(seed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_theta_mini5k, q_theta_mf_mini5k = torch.load('data/dt_1_t_100000_n_1_minibatch/theta_from_multi_x_q_mini5k_scon_c_{}.pt'.format(seed))\n",
    "losses_mini5k, losses_mf_mini5k = torch.load('data/dt_1_t_100000_n_1_minibatch/theta_from_multi_x_loss_mini5k_scon_c_{}.pt'.format(seed))\n",
    "times_mini5k, times_mf_mini5k = torch.load('data/dt_1_t_100000_n_1_minibatch/theta_from_multi_x_time_mini5k_scon_c_{}.pt'.format(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_theta_mini1k, q_theta_mf_mini1k = torch.load('data/dt_1_t_100000_n_1_minibatch/theta_from_multi_x_q_mini1k_scon_c_{}.pt'.format(seed))\n",
    "losses_mini1k, losses_mf_mini1k = torch.load('data/dt_1_t_100000_n_1_minibatch/theta_from_multi_x_loss_mini1k_scon_c_{}.pt'.format(seed))\n",
    "times_mini1k, times_mf_mini1k = torch.load('data/dt_1_t_100000_n_1_minibatch/theta_from_multi_x_time_mini1k_scon_c_{}.pt'.format(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_theta, q_theta_mf = torch.load('data/dt_1_t_1000000_n_1_minibatch/theta_from_x_q_scon_c_{}.pt'.format(seed))\n",
    "losses, losses_mf = torch.load('data/dt_1_t_1000000_n_1_minibatch/theta_from_x_loss_scon_c_{}.pt'.format(seed))\n",
    "times, times_mf = torch.load('data/dt_1_t_1000000_n_1_minibatch/theta_from_x_time_scon_c_{}.pt'.format(seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with unobserved $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_theta_y_file = '../training_pt_outputs/q_theta_iter_310000_t_1000_dt_1.0_batch_40_layers_5_lr_0.0005_sd_scale_0.333_SCON-C_logit_multi_2021_09_22_11_48_46.pt'\n",
    "q_theta_y_mf_file = '../training_pt_outputs/q_theta_iter_250000_t_1000_dt_1.0_batch_45_layers_5_lr_0.0005_sd_scale_0.333_SCON-C_no_CO2_logit_alt_2021_09_23_07_08_19.pt'\n",
    "device=torch.device('cpu')\n",
    "q_theta_y = extract_dist(torch.load(q_theta_y_file, map_location=device))\n",
    "q_theta_y_mf = extract_dist(torch.load(q_theta_y_mf_file, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_file = '../generated_data/SCON-C_CO2_logit_alt_sample_y_t_1000_dt_0-01_sd_scale_0-333_rsample.pt'\n",
    "labels = ['full-rank, observe $y$', 'full-rank, observe $x$', 'mean-field, observe $y$', 'mean-field observe $x$']\n",
    "plot_theta(p_theta, [q_theta_y, q_theta, q_theta_y_mf, q_theta_mf], theta_file, labels, q_theta_raw.keys,\n",
    "           colors=[cm.tab10(1), cm.tab10(1), cm.tab10(2), cm.tab10(2)], linestyles=['-', '--', '-', '--'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr([q_theta_y, q_theta], ['Observe $y$', 'Observe $x$'], q_theta_raw.keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OMGGG I just realized `SBM_SDE` would need to change as well to support theta-from-multiple-xs inference. We need to compute the negative ELBO:\n",
    "$$\\mathcal{L} = - \\frac{1}{S} \\sum_s \\left[ \\log p(\\theta^{(s)}) + \\log p(x|\\theta^{(s)}) - \\log q(\\theta^{(s)}) \\right]$$\n",
    "where $S$ is the batch size.\n",
    "\n",
    "With $M$ sequences of $x_i, i = 1, ..., M$, the log likelihood term is now:\n",
    "$$\\log p(x|\\theta^{(s)}) = \\sum_i \\log p(x_i|\\theta^{(s)})$$\n",
    "\n",
    "So drift diffusion needs to take `C_PATH` of size `(S, M, N, D)` (currently `(S, N, D)`) and return a log likelihood tensor of size `(S, M)` (currently `(S, )`), where $N$ is the number of time steps and $D$ is the state dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(batch_size, 1, 1) \n",
    "diff1 = torch.arange(5).reshape((-1, 1, 1))\n",
    "diff2 = torch.arange(5, 10).reshape((-1, 1, 1))\n",
    "diff3 = torch.arange(10, 15).reshape((-1, 1, 1))\n",
    "diff_list = [diff1, diff2, diff3]\n",
    "diff_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = \n",
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_tensor = torch.diag_embed(torch.sqrt(LowerBound.apply(torch.cat(diff_list, 2), 1e-8))) # (batch_size, 1, state_dim, state_dim)\n",
    "diff_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_tensor[1] == torch.sqrt(torch.diag(torch.tensor([1, 6, 11])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(batch_size, N-1, state_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2, 3).reshape(2, 1, -1)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.diag_embed(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(12).reshape((2, 2, 3))\n",
    "b = torch.arange(4).reshape((2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_minibatches = 4\n",
    "\n",
    "minibatch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5000/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_size = n // 3\n",
    "batch_indices = torch.arange(num_minibatches) * minibatch_size\n",
    "batch_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = x[0]\n",
    "x0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch.randint(num_minibatches, (niter, ))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x_i[max(0, batch_indices[batch] - 1):batch_indices[batch + 1]] for x_i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0[batch_indices[batch0]-1:batch_indices[batch0+1]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.gather(x, 0, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_size = 1000\n",
    "t - minibatch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_indices = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_indices[torch.randint(len(start_indices), (niter, ))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.all(torch.bincount(torch.randint(len(start_indices), (niter, ))) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = torch.randint(100001, (niter, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.bincount(start_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_indices = torch.arange(0, t + 1 - minibatch_size, 100) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_indices[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.arange(0, t - minibatch_size, 100) + 1)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = {1, 2, 3}\n",
    "l = []\n",
    "l.extend(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float('123.45')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
