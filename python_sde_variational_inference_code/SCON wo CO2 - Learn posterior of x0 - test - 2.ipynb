{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "#Torch-related imports\n",
    "import torch\n",
    "import torch.distributions as D\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Function\n",
    "\n",
    "#Model-specific imports\n",
    "from SBM_SDE import *\n",
    "from obs_and_flow import *\n",
    "from training import calc_log_lik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "devi = torch.device(\"\".join([\"cuda:\",f'{cuda_id}']) if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dt_flow = 0.1 #SDE discretization timestep.\n",
    "t = 250 #Simulation run for T hours.\n",
    "n = int(t / dt_flow) + 1\n",
    "t_span = np.linspace(0, t, n)\n",
    "t_span_tensor = torch.reshape(torch.Tensor(t_span), [1, n, 1]) #T_span needs to be converted to tensor object. Additionally, facilitates conversion of I_S and I_D to tensor objects.\n",
    "niter = 11200\n",
    "piter = 200\n",
    "state_dim_SCON = 3 #Not including CO2 in STATE_DIM, because CO2 is an observation.\n",
    "state_dim_SAWB = 4 #Not including CO2 in STATE_DIM, because CO2 is an observation.\n",
    "pretrain_lr = 1e-2\n",
    "train_lr = 1e-3\n",
    "batch_size = 10 #Number of sets of observation outputs to sample per set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_ref = 283\n",
    "temp_rise = 5 #High estimate of 5 celsius temperature rise by 2100. \n",
    "\n",
    "#System parameters from deterministic CON model\n",
    "u_M = 0.002\n",
    "a_SD = 0.33\n",
    "a_DS = 0.33\n",
    "a_M = 0.33\n",
    "a_MSC = 0.5\n",
    "k_S_ref = 0.000025\n",
    "k_D_ref = 0.005\n",
    "k_M_ref = 0.0002\n",
    "Ea_S = 75\n",
    "Ea_D = 50\n",
    "Ea_M = 50\n",
    "\n",
    "#SCON diffusion matrix parameters\n",
    "c_SOC = 1.\n",
    "c_DOC = 0.01\n",
    "c_MBC = 0.05\n",
    "\n",
    "SCON_C_params_dict = {'u_M': u_M, 'a_SD': a_SD, 'a_DS': a_DS, 'a_M': a_M, 'a_MSC': a_MSC, 'k_S_ref': k_S_ref, 'k_D_ref': k_D_ref, 'k_M_ref': k_M_ref, 'Ea_S': Ea_S, 'Ea_D': Ea_D, 'Ea_M': Ea_M, 'c_SOC': c_SOC, 'c_DOC': c_DOC, 'c_MBC': c_MBC}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_error_scale = 0.1\n",
    "\n",
    "x0_SCON = [37, 0.1, 0.9]\n",
    "x0_SCON_tensor = torch.tensor(x0_SCON)\n",
    "x0_prior_SCON = D.multivariate_normal.MultivariateNormal(x0_SCON_tensor,\n",
    "                                                         scale_tril=torch.eye(state_dim_SCON) * obs_error_scale * x0_SCON_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_times, obs_means_CON, obs_error_CON = csv_to_obs_df('y_from_x_t_250_dt_0-01.csv', state_dim_SCON, t, obs_error_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain temperature forcing function.\n",
    "temp_tensor = temp_gen(t_span_tensor, temp_ref, temp_rise)\n",
    "\n",
    "#Obtain SOC and DOC pool litter input vectors for use in flow SDE functions.\n",
    "i_s_tensor = i_s(t_span_tensor) #Exogenous SOC input function\n",
    "i_d_tensor = i_d(t_span_tensor) #Exogenous DOC input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_model_CON_noCO2 = ObsModel(DEVICE = devi, TIMES = obs_times, DT = dt_flow, MU = obs_means_CON, SCALE = obs_error_CON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 51])\n",
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "print(obs_model_CON_noCO2.mu.shape)\n",
    "print(obs_model_CON_noCO2.scale.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_log_lik(C_PATH, T_SPAN_TENSOR, DT, I_S_TENSOR, I_D_TENSOR, TEMP_TENSOR, TEMP_REF, DRIFT_DIFFUSION, X0_PRIOR, PARAMS_DICT):\n",
    "    drift, diffusion_sqrt = DRIFT_DIFFUSION(C_PATH[:, :-1, :], T_SPAN_TENSOR[:, :-1, :], I_S_TENSOR[:, :-1, :], I_D_TENSOR[:, :-1, :], TEMP_TENSOR[:, :-1, :], TEMP_REF, PARAMS_DICT)\n",
    "    euler_maruyama_state_sample_object = D.multivariate_normal.MultivariateNormal(loc = C_PATH[:, :-1, :] + drift * DT, scale_tril = diffusion_sqrt * math.sqrt(DT))\n",
    "    \n",
    "    # Compute log p(x|theta) = log p(x|x0, theta) + log p(x0|theta)\n",
    "    ll = euler_maruyama_state_sample_object.log_prob(C_PATH[:, 1:, :]).sum(-1) # log p(x|x0, theta)\n",
    "    ll += X0_PRIOR.log_prob(C_PATH[:, 0, :]) # log p(x0|theta)\n",
    "    \n",
    "    return ll # (batch_size, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(DEVICE, PRETRAIN_LR, TRAIN_LR, NITER, PRETRAIN_ITER, BATCH_SIZE, OBS_MODEL,\n",
    "          STATE_DIM, T, DT, N, T_SPAN_TENSOR, I_S_TENSOR, I_D_TENSOR, TEMP_TENSOR, TEMP_REF,\n",
    "          DRIFT_DIFFUSION, X0_PRIOR, PARAMS_DICT,\n",
    "          LEARN_PARAMS = False, LR_DECAY = 0.1, DECAY_STEP_SIZE = 1000, PRINT_EVERY = 500):\n",
    "    net = SDEFlow(DEVICE, OBS_MODEL, STATE_DIM, T, DT, N, num_layers = 7).to(DEVICE)\n",
    "    optimizer = optim.Adam(net.parameters(), lr = PRETRAIN_LR)\n",
    "    \n",
    "    if LEARN_PARAMS:\n",
    "        theta_post = MeanField(PARAMS_DICT)\n",
    "        theta_prior = D.normal.Normal(torch.zeros_like(theta_post.means),\n",
    "                                      torch.ones_like(theta_post.std))\n",
    "    if PRETRAIN_ITER >= NITER:\n",
    "        raise Exception(\"PRETRAIN_ITER must be < NITER.\")\n",
    "    best_loss_norm = 1e10\n",
    "    best_loss_ELBO = 1e10\n",
    "    norm_losses = [] #[best_loss_norm] * 10 \n",
    "    ELBO_losses = [] #[best_loss_ELBO] * 10\n",
    "    #C0 = ANALYTICAL_STEADY_STATE_INIT(I_S_TENSOR[0, 0, 0].item(), I_D_TENSOR[0, 0, 0].item(), PARAMS_DICT) #Calculate deterministic initial conditions.\n",
    "    #C0 = C0[(None,) * 2].repeat(BATCH_SIZE, 1, 1).to(DEVICE) #Assign initial conditions to C_PATH.\n",
    "    \n",
    "    with tqdm(total = NITER, desc = f'Train Diffusion', position = -1) as tq:\n",
    "        for it in range(NITER):\n",
    "            net.train()\n",
    "            optimizer.zero_grad()\n",
    "            C_PATH, log_prob = net(BATCH_SIZE) #Obtain paths with solutions at times after t0.\n",
    "            #C_PATH = torch.cat([C0, C_PATH], 1) #Append deterministic CON initial conditions conditional on parameter values to C path. \n",
    "            \n",
    "            if it < PRETRAIN_ITER:\n",
    "                l1_norm_element = C_PATH - torch.mean(OBS_MODEL.mu[:3], -1)\n",
    "                l1_norm = torch.sum(torch.abs(l1_norm_element)).mean()\n",
    "                best_loss_norm = l1_norm if l1_norm < best_loss_norm else best_loss_norm\n",
    "                norm_losses.append(l1_norm.item())\n",
    "                #l2_norm_element = C_PATH - torch.mean(OBS_MODEL.mu, -1)\n",
    "                #l2_norm = torch.sqrt(torch.sum(torch.square(l2_norm_element))).mean()\n",
    "                #best_loss_norm = l2_norm if l2_norm < best_loss_norm else best_loss_norm\n",
    "                #norm_losses.append(l2_norm.item())\n",
    "                \n",
    "                if (it + 1) % PRINT_EVERY == 0:\n",
    "                    print(f\"Moving average norm loss at {it + 1} iterations is: {sum(norm_losses[-10:]) / len(norm_losses[-10:])}. Best norm loss value is: {best_loss_norm}.\")\n",
    "                    print('\\nC_PATH mean =', C_PATH.mean(-2))\n",
    "                    print('\\nC_PATH =', C_PATH)\n",
    "                l1_norm.backward()\n",
    "                #l2_norm.backward()\n",
    "                \n",
    "            else:\n",
    "                if LEARN_PARAMS:\n",
    "                    theta_dict, theta, log_q_theta = theta_post()\n",
    "                    log_p_theta = theta_prior.log_prob(theta).sum(-1)\n",
    "                else:\n",
    "                    theta_dict = PARAMS_DICT\n",
    "                    log_q_theta, log_p_theta = torch.zeros(2)\n",
    "                log_lik = calc_log_lik(C_PATH, T_SPAN_TENSOR.to(DEVICE), DT, I_S_TENSOR.to(DEVICE), I_D_TENSOR.to(DEVICE),\n",
    "                                       TEMP_TENSOR, TEMP_REF, DRIFT_DIFFUSION, X0_PRIOR, theta_dict)\n",
    "                \n",
    "                # - log p(theta) + log q(theta) + log q(x|theta) - log p(x|theta) - log p(y|x, theta)\n",
    "                ELBO = -log_p_theta.mean() + log_q_theta.mean() - log_lik.mean() - OBS_MODEL(C_PATH, theta_dict) + log_prob.mean()\n",
    "                best_loss_ELBO = ELBO if ELBO < best_loss_ELBO else best_loss_ELBO\n",
    "                ELBO_losses.append(ELBO.item())\n",
    "\n",
    "                if (it + 1) % PRINT_EVERY == 0:\n",
    "                    print(f\"Moving average ELBO loss at {it + 1} iterations is: {sum(ELBO_losses[-10:]) / len(ELBO_losses[-10:])}. Best ELBO loss value is: {best_loss_ELBO}.\")\n",
    "                    print('\\nC_PATH mean =', C_PATH.mean(-2))\n",
    "                    print('\\n C_PATH =', C_PATH)\n",
    "                    print(theta_dict)\n",
    "                ELBO.backward()\n",
    "                \n",
    "            torch.nn.utils.clip_grad_norm_(net.parameters(), 3.0)\n",
    "            if it == PRETRAIN_ITER:\n",
    "                optimizer.param_groups[0]['lr'] = TRAIN_LR\n",
    "            elif it % DECAY_STEP_SIZE == 0 and it > PRETRAIN_ITER:\n",
    "                optimizer.param_groups[0]['lr'] *= LR_DECAY\n",
    "            optimizer.step()\n",
    "            tq.update()\n",
    "            \n",
    "    return net, ELBO_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_post(x, obs_model, num_layers, niter, dt, state_idx=0, num_samples=1,\n",
    "              ymin=None, ymax=None):\n",
    "    #net.eval()\n",
    "    #x, _ = net(num_samples)\n",
    "    #x0 = x0[(None,) * 2].repeat(num_samples, 1, 1)\n",
    "    #x = torch.cat((x0, x), 1)\n",
    "    \n",
    "    q_mean, q_std = x[:, :, state_idx].mean(0).detach(), x[:, :, state_idx].std(0).detach()\n",
    "    hours = torch.arange(0, t + dt, dt)\n",
    "    plt.plot(hours, q_mean, label='Posterior mean')\n",
    "    plt.fill_between(hours, q_mean - 2*q_std, q_mean + 2*q_std, alpha=0.5,\n",
    "                     label='Posterior $\\\\mu \\pm 2\\sigma_x$')\n",
    "    plt.plot(obs_model.times, obs_model.mu[state_idx, :], linestyle='None', marker='o',\n",
    "             label='Observed')\n",
    "    plt.fill_between(obs_model.times, obs_model.mu[state_idx, :] - 2 * obs_model.scale[:, state_idx], obs_model.mu[state_idx, :] + 2 * obs_model.scale[:, state_idx], alpha=0.5, label = 'Observation $\\\\mu \\pm 2\\sigma_y$')\n",
    "    \n",
    "    state = ['SOC', 'DOC', 'MBC'][state_idx]\n",
    "    plt.legend()\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylabel(state)\n",
    "    plt.ylim((ymin, ymax))\n",
    "    #plt.title(f'Approximate posterior $q(x|\\\\theta, y)$\\nNumber of samples = {num_samples}\\nTimestep = {dt}\\nIterations = {niter}')\n",
    "    plt.title(f'Timestep = {dt}\\nIterations = {niter}')\n",
    "    plt.savefig(f'net_t_{t}_dt_{dt_flow}_layers_{num_layers}_iter{niter}_{state}.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Diffusion:   0%|          | 0/11200 [00:00<?, ?it/s]\u001b[A\n",
      "Train Diffusion:   0%|          | 1/11200 [00:16<51:23:05, 16.52s/it]\u001b[A\n",
      "Train Diffusion:   0%|          | 2/11200 [00:29<44:46:07, 14.39s/it]\u001b[A\n",
      "Train Diffusion:   0%|          | 3/11200 [00:42<42:36:51, 13.70s/it]\u001b[A\n",
      "Train Diffusion:   0%|          | 4/11200 [00:55<42:30:10, 13.67s/it]\u001b[A\n",
      "Train Diffusion:   0%|          | 5/11200 [01:10<43:27:01, 13.97s/it]\u001b[A\n",
      "Train Diffusion:   0%|          | 6/11200 [01:36<49:50:30, 16.03s/it]\u001b[A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-6f1bde217374>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m net_batch_10_dt_flow_0_1, elbo_hist_batch_10_dt_flow_0_1 = train(devi, pretrain_lr, train_lr, niter, piter, batch_size, obs_model_CON_noCO2,\n\u001b[0m\u001b[1;32m      2\u001b[0m                        \u001b[0mstate_dim_SCON\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt_flow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_span_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_s_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_d_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_ref\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                        \u001b[0mdrift_diffusion_SCON_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0_prior_SCON\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSCON_C_params_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                        LR_DECAY = 0.1, DECAY_STEP_SIZE = 5000, PRINT_EVERY = 20)\n",
      "\u001b[0;32m<ipython-input-20-9ad80d38c137>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(DEVICE, PRETRAIN_LR, TRAIN_LR, NITER, PRETRAIN_ITER, BATCH_SIZE, OBS_MODEL, STATE_DIM, T, DT, N, T_SPAN_TENSOR, I_S_TENSOR, I_D_TENSOR, TEMP_TENSOR, TEMP_REF, DRIFT_DIFFUSION, X0_PRIOR, PARAMS_DICT, LEARN_PARAMS, LR_DECAY, DECAY_STEP_SIZE, PRINT_EVERY)\u001b[0m\n\u001b[1;32m     40\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nC_PATH mean ='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_PATH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nC_PATH ='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0ml1_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                 \u001b[0;31m#l2_norm.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0m_is_legacy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;31m# _forward_cls is defined by derived class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net_batch_10_dt_flow_0_1, elbo_hist_batch_10_dt_flow_0_1 = train(devi, pretrain_lr, train_lr, niter, piter, batch_size, obs_model_CON_noCO2,\n",
    "                       state_dim_SCON, t, dt_flow, n, t_span_tensor, i_s_tensor, i_d_tensor, temp_tensor, temp_ref,\n",
    "                       drift_diffusion_SCON_C, x0_prior_SCON, SCON_C_params_dict,\n",
    "                       LR_DECAY = 0.1, DECAY_STEP_SIZE = 5000, PRINT_EVERY = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
